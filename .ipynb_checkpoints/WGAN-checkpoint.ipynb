{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dasha/src/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/dasha/src/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/dasha/src/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/dasha/src/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/dasha/src/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/dasha/src/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn.datasets\n",
    "\n",
    "\n",
    "import tflib as lib\n",
    "import tflib.ops.linear\n",
    "import tflib.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'wgan-gp' # wgan or wgan-gp\n",
    "DATASET = 'gaussian' # 8gaussians, 25gaussians, swissroll\n",
    "DIM = 64 # Model dimensionality (number of neurons in the hidden layer(s))\n",
    "FIXED_GENERATOR = False # whether to hold the generator fixed at real data plus\n",
    "                        # Gaussian noise, as in the plots in the paper\n",
    "LAMBDA = .1 # Smaller lambda makes things faster for toy tasks, but isn't\n",
    "            # necessary if you increase CRITIC_ITERS enough\n",
    "CRITIC_ITERS = 5 # How many critic iterations per generator iteration\n",
    "BATCH_SIZE = 256 # Batch size\n",
    "ITERS = 2500#100000 # how many generator iterations to train for\n",
    "DATA_DIM = 32\n",
    "LATENT_DIM = 4\n",
    "INITIALIZATION = 'he'#'glorot'\n",
    "COVARIANCE_SCALE = np.sqrt(DATA_DIM)\n",
    "INITIALIZE_LAST = True\n",
    "SAMPLE_SIZE = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase local vars:\n",
      "\tBATCH_SIZE: 256\n",
      "\tCOVARIANCE_SCALE: 5.656854249492381\n",
      "\tCRITIC_ITERS: 5\n",
      "\tDATASET: gaussian\n",
      "\tDATA_DIM: 32\n",
      "\tDIM: 64\n",
      "\tFIXED_GENERATOR: False\n",
      "\tINITIALIZATION: he\n",
      "\tINITIALIZE_LAST: True\n",
      "\tITERS: 2500\n",
      "\tLAMBDA: 0.1\n",
      "\tLATENT_DIM: 4\n",
      "\tMODE: wgan-gp\n",
      "\tSAMPLE_SIZE: 100000\n"
     ]
    }
   ],
   "source": [
    "lib.print_model_settings(locals().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not needed anymore\n",
    "ML_cov = np.diag(np.concatenate([np.ones(LATENT_DIM), np.zeros(DATA_DIM-LATENT_DIM)]))/COVARIANCE_SCALE\n",
    "def get_cov_diff(fake_sample):\n",
    "    \"\"\"\n",
    "    Outputs frobenius norm of the difference between generated data covariance and ML distribution covariance.\n",
    "    \"\"\"\n",
    "    generated_cov = np.cov(fake_sample.T)\n",
    "    cov_mismatch = np.linalg.norm(generated_cov - ML_cov)\n",
    "    return cov_mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_last = INITIALIZATION if INITIALIZE_LAST else None\n",
    "\n",
    "def ReLULayer(name, n_in, n_out, inputs):\n",
    "    output = lib.ops.linear.Linear(\n",
    "        name+'.Linear',\n",
    "        n_in,\n",
    "        n_out,\n",
    "        inputs,\n",
    "        initialization=INITIALIZATION\n",
    "    )\n",
    "    output = tf.nn.relu(output)\n",
    "    return output\n",
    "\n",
    "def Generator(n_samples, real_data):\n",
    "    if FIXED_GENERATOR:\n",
    "        return real_data + (1.*tf.random_normal(tf.shape(real_data)))\n",
    "    else:\n",
    "        noise = tf.random_normal((n_samples, LATENT_DIM))\n",
    "        output = ReLULayer('Generator1', LATENT_DIM, DIM, noise)\n",
    "        output = ReLULayer('Generator2', DIM, DIM, output)\n",
    "        output = ReLULayer('Generator3', DIM, DIM, output)\n",
    "        output = lib.ops.linear.Linear('Generator4', DIM, DATA_DIM, output, initialization=init_last)#MAYBE THEY DIDN'T DO IT\n",
    "        return output\n",
    "\n",
    "def Discriminator(inputs):\n",
    "    output = ReLULayer('Discriminator1', 32, DIM, inputs)\n",
    "    output = ReLULayer('Discriminator2', DIM, DIM, output)\n",
    "    output = ReLULayer('Discriminator3', DIM, DIM, output)\n",
    "    output = lib.ops.linear.Linear('Discriminator4', DIM, 1, output, initialization=init_last)\n",
    "    return tf.reshape(output, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dasha/src/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "real_data = tf.placeholder(tf.float32, shape=[None, DATA_DIM])\n",
    "n_samples = BATCH_SIZE\n",
    "fake_data = Generator(n_samples, real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_real = Discriminator(real_data)\n",
    "disc_fake = Discriminator(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN loss\n",
    "disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)\n",
    "gen_cost = -tf.reduce_mean(disc_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dasha/src/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# WGAN gradient penalty\n",
    "if MODE == 'wgan-gp':\n",
    "    alpha = tf.random_uniform(\n",
    "        shape=[BATCH_SIZE,1], \n",
    "        minval=0.,\n",
    "        maxval=1.\n",
    "    )\n",
    "    interpolates = alpha*real_data + ((1-alpha)*fake_data)\n",
    "    disc_interpolates = Discriminator(interpolates)\n",
    "    gradients = tf.gradients(disc_interpolates, [interpolates])[0]\n",
    "    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "    gradient_penalty = tf.reduce_mean((slopes-1)**2)\n",
    " \n",
    "    disc_cost += LAMBDA*gradient_penalty\n",
    "\n",
    "disc_params = lib.params_with_name('Discriminator')\n",
    "gen_params = lib.params_with_name('Generator')\n",
    "\n",
    "if MODE == 'wgan-gp':\n",
    "    disc_train_op = tf.train.AdamOptimizer(\n",
    "        learning_rate=1e-4, \n",
    "        beta1=0.5, \n",
    "        beta2=0.9\n",
    "    ).minimize(\n",
    "        disc_cost, \n",
    "        var_list=disc_params\n",
    "    )\n",
    "    if len(gen_params) > 0:\n",
    "        gen_train_op = tf.train.AdamOptimizer(\n",
    "            learning_rate=1e-4, \n",
    "            beta1=0.5, \n",
    "            beta2=0.9\n",
    "        ).minimize(\n",
    "            gen_cost, \n",
    "            var_list=gen_params\n",
    "        )\n",
    "    else:\n",
    "        gen_train_op = tf.no_op()\n",
    "\n",
    "else:\n",
    "    disc_train_op = tf.train.RMSPropOptimizer(learning_rate=5e-5).minimize(\n",
    "        disc_cost, \n",
    "        var_list=disc_params\n",
    "    )\n",
    "    if len(gen_params) > 0:\n",
    "        gen_train_op = tf.train.RMSPropOptimizer(learning_rate=5e-5).minimize(\n",
    "            gen_cost, \n",
    "            var_list=gen_params\n",
    "        )\n",
    "    else:\n",
    "        gen_train_op = tf.no_op()\n",
    "\n",
    "\n",
    "    # Build an op to do the weight clipping\n",
    "    clip_ops = []\n",
    "    for var in disc_params:\n",
    "        clip_bounds = [-.01, .01]\n",
    "        clip_ops.append(\n",
    "            tf.assign(\n",
    "                var, \n",
    "                tf.clip_by_value(var, clip_bounds[0], clip_bounds[1])\n",
    "            )\n",
    "        )\n",
    "    clip_disc_weights = tf.group(*clip_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator params:\n",
      "\tGenerator1.Linear/Generator1.Linear.W:0\t(4, 64)\n",
      "\tGenerator1.Linear/Generator1.Linear.b:0\t(64,)\n",
      "\tGenerator2.Linear/Generator2.Linear.W:0\t(64, 64)\n",
      "\tGenerator2.Linear/Generator2.Linear.b:0\t(64,)\n",
      "\tGenerator3.Linear/Generator3.Linear.W:0\t(64, 64)\n",
      "\tGenerator3.Linear/Generator3.Linear.b:0\t(64,)\n",
      "\tGenerator4/Generator4.W:0\t(64, 32)\n",
      "\tGenerator4/Generator4.b:0\t(32,)\n",
      "Discriminator params:\n",
      "\tDiscriminator1.Linear/Discriminator1.Linear.W:0\t(32, 64)\n",
      "\tDiscriminator1.Linear/Discriminator1.Linear.b:0\t(64,)\n",
      "\tDiscriminator2.Linear/Discriminator2.Linear.W:0\t(64, 64)\n",
      "\tDiscriminator2.Linear/Discriminator2.Linear.b:0\t(64,)\n",
      "\tDiscriminator3.Linear/Discriminator3.Linear.W:0\t(64, 64)\n",
      "\tDiscriminator3.Linear/Discriminator3.Linear.b:0\t(64,)\n",
      "\tDiscriminator4/Discriminator4.W:0\t(64, 1)\n",
      "\tDiscriminator4/Discriminator4.b:0\t(1,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Generator params:\")\n",
    "for var in lib.params_with_name('Generator'):\n",
    "    print(\"\\t{}\\t{}\".format(var.name, var.get_shape()))\n",
    "print(\"Discriminator params:\")\n",
    "for var in lib.params_with_name('Discriminator'):\n",
    "    print(\"\\t{}\\t{}\".format(var.name, var.get_shape()))\n",
    "\n",
    "frame_index = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_train_gen():\n",
    "    if DATASET == '25gaussians':\n",
    "    \n",
    "        dataset = []\n",
    "        for i in xrange(100000/25):\n",
    "            for x in xrange(-2, 3):\n",
    "                for y in xrange(-2, 3):\n",
    "                    point = np.random.randn(2)*0.05\n",
    "                    point[0] += 2*x\n",
    "                    point[1] += 2*y\n",
    "                    dataset.append(point)\n",
    "        dataset = np.array(dataset, dtype='float32')\n",
    "        np.random.shuffle(dataset)\n",
    "        dataset /= 2.828 # stdev\n",
    "        while True:\n",
    "            for i in xrange(len(dataset)/BATCH_SIZE):\n",
    "                yield dataset[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "\n",
    "    elif DATASET == 'swissroll':\n",
    "\n",
    "        while True:\n",
    "            data = sklearn.datasets.make_swiss_roll(\n",
    "                n_samples=BATCH_SIZE, \n",
    "                noise=0.25\n",
    "            )[0]\n",
    "            data = data.astype('float32')[:, [0, 2]]\n",
    "            data /= 7.5 # stdev plus a little\n",
    "            yield data\n",
    "\n",
    "    elif DATASET == '8gaussians':\n",
    "    \n",
    "        scale = 2.\n",
    "        centers = [\n",
    "            (1,0),\n",
    "            (-1,0),\n",
    "            (0,1),\n",
    "            (0,-1),\n",
    "            (1./np.sqrt(2), 1./np.sqrt(2)),\n",
    "            (1./np.sqrt(2), -1./np.sqrt(2)),\n",
    "            (-1./np.sqrt(2), 1./np.sqrt(2)),\n",
    "            (-1./np.sqrt(2), -1./np.sqrt(2))\n",
    "        ]\n",
    "        centers = [(scale*x,scale*y) for x,y in centers]\n",
    "        while True:\n",
    "            dataset = []\n",
    "            for i in xrange(BATCH_SIZE):\n",
    "                point = np.random.randn(2)*.02\n",
    "                center = random.choice(centers)\n",
    "                point[0] += center[0]\n",
    "                point[1] += center[1]\n",
    "                dataset.append(point)\n",
    "            dataset = np.array(dataset, dtype='float32')\n",
    "            dataset /= 1.414 # stdev\n",
    "            yield dataset\n",
    "\n",
    "    elif DATASET == 'gaussian':\n",
    "        np.random.seed(1)\n",
    "        full_dataset = np.random.randn(SAMPLE_SIZE,DATA_DIM) / np.sqrt(COVARIANCE_SCALE) \n",
    "        i = 0\n",
    "        offset = 0\n",
    "        while True:\n",
    "            dataset = full_dataset[i*BATCH_SIZE+offset:(i+1)*BATCH_SIZE+offset,:]\n",
    "            if (i+1)*BATCH_SIZE+offset > SAMPLE_SIZE: \n",
    "                offset = (i+1)*BATCH_SIZE+offset - SAMPLE_SIZE\n",
    "                np.random.shuffle(full_dataset)\n",
    "                dataset = np.concatenate([dataset,full_dataset[:offset,:]], axis = 0)\n",
    "                i = -1 \n",
    "            i+=1\n",
    "            yield dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_dataset = np.random.randn(SAMPLE_SIZE,DATA_DIM) / np.sqrt(COVARIANCE_SCALE)  #data_covariance = np.cov(full_dataset.T)\n",
    "#w, v = np.linalg.eigh(data_covariance)\n",
    "#w[:-LATENT_DIM] = 0\n",
    "#ML_covariance = v.dot(np.diag(w)).dot(v.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fake_data = tf.reduce_mean(fake_data, axis=0, keep_dims=True)\n",
    "vx = tf.matmul(tf.transpose(fake_data),fake_data)/tf.cast(tf.shape(fake_data)[0]-1, tf.float32)\n",
    "mx = tf.matmul(tf.transpose(mean_fake_data), mean_fake_data)\n",
    "fake_data_covariance = vx - mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_covariance = np.eye(DATA_DIM)/COVARIANCE_SCALE\n",
    "accuracy_metric = tf.norm(fake_data_covariance - ML_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2682\tdisc cost\t-10.94989013671875\taccuracy\t9.667925834655762\n",
      "iter 2782\tdisc cost\t-4.1765456199646\taccuracy\t3.5673017501831055\n",
      "iter 2882\tdisc cost\t-1.1790003776550293\taccuracy\t1.7334858179092407\n",
      "iter 2982\tdisc cost\t-0.9120767712593079\taccuracy\t1.4301424026489258\n",
      "iter 3082\tdisc cost\t-0.9597740769386292\taccuracy\t1.6911163330078125\n",
      "iter 3182\tdisc cost\t-0.9088401794433594\taccuracy\t1.6144644021987915\n",
      "iter 3282\tdisc cost\t-0.9544743895530701\taccuracy\t1.5705102682113647\n",
      "iter 3382\tdisc cost\t-0.8334892988204956\taccuracy\t1.3866177797317505\n",
      "iter 3482\tdisc cost\t-0.6678664684295654\taccuracy\t1.2948089838027954\n",
      "iter 3582\tdisc cost\t-0.7454020977020264\taccuracy\t1.3896644115447998\n",
      "iter 3682\tdisc cost\t-0.9207289814949036\taccuracy\t1.3889362812042236\n",
      "iter 3782\tdisc cost\t-0.8401126265525818\taccuracy\t1.4567564725875854\n",
      "iter 3882\tdisc cost\t-0.8120813965797424\taccuracy\t1.178572416305542\n",
      "iter 3982\tdisc cost\t-0.6837441325187683\taccuracy\t1.228823184967041\n",
      "iter 4082\tdisc cost\t-0.6702259182929993\taccuracy\t1.1637498140335083\n",
      "iter 4182\tdisc cost\t-0.6530694365501404\taccuracy\t1.144351601600647\n",
      "iter 4282\tdisc cost\t-0.6118537783622742\taccuracy\t1.1416794061660767\n",
      "iter 4382\tdisc cost\t-0.6052162051200867\taccuracy\t1.114530324935913\n",
      "iter 4482\tdisc cost\t-0.5929452776908875\taccuracy\t1.1259843111038208\n",
      "iter 4582\tdisc cost\t-0.6020004153251648\taccuracy\t1.0072718858718872\n",
      "iter 4682\tdisc cost\t-0.5585894584655762\taccuracy\t1.1093580722808838\n",
      "iter 4782\tdisc cost\t-0.5989184379577637\taccuracy\t1.3235654830932617\n",
      "iter 4882\tdisc cost\t-0.5999842286109924\taccuracy\t1.303830623626709\n",
      "iter 4982\tdisc cost\t-0.5352962613105774\taccuracy\t1.1338317394256592\n",
      "iter 5082\tdisc cost\t-0.49814215302467346\taccuracy\t1.0915741920471191\n"
     ]
    }
   ],
   "source": [
    "# Train loop!\n",
    "accuracy_history = []\n",
    "#if COVARIANCE_SCALE == DATA_DIM:\n",
    "#    model_name = \"d_\"\n",
    "#else:\n",
    "#    model_name = \"root_d_\"\n",
    "#model_name = model_name + \"initialize_last_\" + str(INITIALIZE_LAST) + \"_initialization_\" + INITIALIZATION\n",
    "if MODE == 'wgan-gp':\n",
    "    model_name = \"WGAN_GP\"\n",
    "else:\n",
    "    model_name = \"WGAN_WC\"\n",
    "model_name = model_name + \"_LATENT_DIM_\" + str(LATENT_DIM) + \"_initialization_\" + INITIALIZATION\n",
    "model_name = 'tmp_'+model_name #not to spoil the saved results\n",
    "plt.figure()\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    gen = inf_train_gen()\n",
    "    for iteration in range(ITERS):\n",
    "        # Train generator\n",
    "        if iteration > 0:\n",
    "            _ = session.run(gen_train_op)\n",
    "        # Train critic\n",
    "        for i in range(CRITIC_ITERS):\n",
    "            _data = next(gen)\n",
    "            _disc_cost, _, accuracy, sample = session.run(\n",
    "                [disc_cost, disc_train_op, accuracy_metric, fake_data],\n",
    "                feed_dict={real_data: _data}\n",
    "            )\n",
    "            if MODE == 'wgan':\n",
    "                _ = session.run([clip_disc_weights])\n",
    "        # Write logs and save samples\n",
    "        #print(np.abs(get_cov_diff(fake_sample)-accuracy)/get_cov_diff(fake_sample))\n",
    "        lib.plot.plot('disc cost', _disc_cost)\n",
    "        lib.plot.plot('accuracy', accuracy)\n",
    "        lib.plot.plot('sample', sample)\n",
    "        \n",
    "        accuracy_history.append(accuracy)\n",
    "        if iteration % 100 == 99:\n",
    "            lib.plot.flush(\"./\"+model_name+\".pkl\")\n",
    "            plt.clf()\n",
    "            plt.grid(\"on\", \"both\")\n",
    "            plt.plot(np.arange(iteration+1), accuracy_history)\n",
    "            plt.plot(np.arange(iteration+1), np.zeros(iteration+1))\n",
    "            plt.savefig(\"./accuracy_history_\"+model_name+\".png\")\n",
    "        lib.plot.tick()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- \"accuracy_history_rootd_covariance_he_gp_inithelastlayer.png\" -- $K_Y = I/\\sqrt{D}$, output layer of generator and discriminator was initialized with he initializations cost = \n",
    "\n",
    "-- \"accuracy_history_rootd_covariance_he_gp_initnonelastlayer.png\" -- $K_Y = I/\\sqrt{D}$, output layer of generator and discriminator were initialized with random orthogonal matrices cost = \n",
    "\n",
    "-- \"accuracy_history_d_covariance_he_gp_initnonelastlayer.png\" -- $K_Y = I/D$, output layer of generator and discriminator were initialized with random orthogonal matrices cost = 0.19\n",
    "\n",
    "ADD: $K_Y = I/D$ and error in $K_Y:$ Y is generated with $I/d$ and $K_Y = I/\\sqrt{D}$ for ML matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5083\t\n"
     ]
    }
   ],
   "source": [
    "step = 10\n",
    "plt.clf()\n",
    "plt.grid(\"on\", \"both\")\n",
    "accuracy_history = np.array(accuracy_history)\n",
    "idx = np.max(np.where(accuracy_history > 3.5))\n",
    "plt.plot(np.arange(idx,iteration+1,step), accuracy_history[idx::step])\n",
    "plt.plot(np.arange(iteration+1), np.zeros(iteration+1))\n",
    "plt.savefig(\"./accuracy_history_\"+model_name+\".png\")\n",
    "lib.plot.flush(\"./\"+model_name+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('./log.pkl', 'rb') as f:\n",
    "    dict_out = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = map(np.array,zip(*list(dict_out['accuracy'].items())))\n",
    "if np.max(y) <= 3.5:\n",
    "    idx = 0\n",
    "\n",
    "idx = np.max(np.where(y>3.5))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 50\n",
    "plt.figure()\n",
    "plt.plot(x[idx::step], y[idx::step], linewidth=2, color = 'red')\n",
    "plt.plot(np.arange(np.max(x)+1), np.zeros(np.max(x)+1), linewidth=2, color = 'green')\n",
    "plt.grid('on', 'both')\n",
    "plt.savefig('accuracy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 50\n",
    "plt.clf()\n",
    "plt.grid(\"on\", \"both\")\n",
    "accuracy_history = np.array(accuracy_history)\n",
    "idx = np.max(np.where(accuracy_history > 3.5))\n",
    "plt.plot(np.arange(idx,iteration+1,step), accuracy_history[idx::step], linewidth=2, color = 'red')\n",
    "plt.plot(np.arange(iteration+1), np.zeros(iteration+1), linewidth=2, color = 'green')\n",
    "plt.savefig(\"./accuracy_history_\"+model_name+\".png\")\n",
    "lib.plot.flush(\"./\"+model_name+\".pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
