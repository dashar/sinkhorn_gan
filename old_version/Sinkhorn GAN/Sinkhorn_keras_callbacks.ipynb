{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 64 # Model dimensionality (number of neurons in the hidden layer(s))\n",
    "CRITIC_ITERS = 50 # How many critic iterations (Sinkhorn iterations) per generator iteration#was 50\n",
    "BATCH_SIZE = 256 # Batch size\n",
    "ITERS = 2500#100000 # how many generator iterations to train for\n",
    "DATA_DIM = 32\n",
    "LATENT_DIM = 4\n",
    "INITIALIZATION = 'he'#'glorot'\n",
    "COVARIANCE_SCALE = np.sqrt(DATA_DIM)\n",
    "INITIALIZE_LAST = True\n",
    "SAMPLE_SIZE = 100000\n",
    "LAMBDA = 0.3#2/(COVARIANCE_SCALE)\n",
    "MODE = 'divergence' #'loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INITIALIZATION == 'he':\n",
    "    weight_initializer = keras.initializers.he_uniform()\n",
    "if INITIALIZATION == 'glorot':\n",
    "    weight_initializer = keras.initializers.glorot_uniform()\n",
    "bias_initializer = keras.initializers.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special \n",
    "def logsumexp(x, axis = None):\n",
    "    if axis is None:\n",
    "        val = scipy.special.logsumexp(x)\n",
    "        if val != val:\n",
    "            return np.max(x)\n",
    "    val = scipy.special.logsumexp(x, axis = axis)\n",
    "#    val = np.nan_to_num(val, -5)\n",
    "#     idx = np.where(val!=val)[0]\n",
    "#     if len(idx) == 0:\n",
    "#         return val\n",
    "#     print(len(idx))\n",
    "#     if axis == 0:\n",
    "#         val[idx] = np.max(x[:, idx], axis = 0)\n",
    "#     if axis == 1:\n",
    "#         val[idx] = np.max(x[idx, :], axis = 1)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_l2_matrix_np(X, Y):\n",
    "    squared_l2_X = np.sum(np.square(X), axis = 1, keepdims=True)\n",
    "    squared_l2_Y = np.sum(np.square(Y), axis = 1, keepdims=True)\n",
    "    XY = X.dot(Y.T)\n",
    "    return squared_l2_X + squared_l2_Y.T - 2 * XY\n",
    "\n",
    "def log_coupling_np(psi_X, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon):\n",
    "    C_tild = cost_matrix - np.expand_dims(psi_X, axis=1) - np.expand_dims(psi_Y, axis=0)\n",
    "    return -C_tild/epsilon + log_X_prob + log_Y_prob\n",
    "\n",
    "def sinkhorn_step_np(psi_X, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon, return_diff = False):\n",
    "    #print(psi_X, psi_Y)\n",
    "    K_tld = np.exp(log_coupling_np(psi_X, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon)- log_X_prob -log_Y_prob)    \n",
    "    Ku = np.sum(K_tld, axis = 1)\n",
    "    #print(Ku)\n",
    "    psi_X_upd = epsilon * (log_X_prob - np.log(Ku + 10**(-6)))+psi_X\n",
    "    K_tld = np.exp(log_coupling_np(psi_X_upd, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon)-log_X_prob-log_Y_prob)  \n",
    "    Kv = np.sum(K_tld, axis = 0)\n",
    "    #print(Kv)\n",
    "    psi_Y_upd = epsilon * (log_Y_prob - np.log(Kv + 10**(-6)))+psi_Y\n",
    "    \n",
    "    #psi_X_upd = - epsilon * logsumexp(log_Y_prob+(np.expand_dims(psi_Y, axis=0) - cost_matrix) / epsilon, axis = 1)\n",
    "    #psi_Y_upd = - epsilon * logsumexp(log_X_prob+(np.expand_dims(psi_X_upd, axis=1) - cost_matrix)/epsilon,axis = 0)\n",
    "    if return_diff:\n",
    "        diff = np.linalg.norm(psi_X_upd - psi_X) + np.linalg.norm(psi_Y_upd - psi_Y)\n",
    "    psi_X = psi_X_upd\n",
    "    psi_Y = psi_Y_upd\n",
    "    if return_diff:\n",
    "        return psi_X, psi_Y, diff\n",
    "    return psi_X, psi_Y\n",
    "\n",
    "def sinkhorn_step_symm_np(psi_X, log_X_prob, cost_matrix, epsilon, return_diff = False):\n",
    "    psi_X_upd = (psi_X - epsilon * logsumexp(log_X_prob+(np.expand_dims(psi_X, axis=0) - \n",
    "                                                         cost_matrix) / epsilon, axis = 1))/2\n",
    "    if return_diff:\n",
    "        diff = np.linalg.norm(psi_X_upd - psi_X) \n",
    "    psi_X = psi_X_upd\n",
    "    if return_diff:\n",
    "        return psi_X, diff\n",
    "    return psi_X\n",
    "\n",
    "def sinkhorn_loss_np(X, Y, epsilon, num_steps):\n",
    "    cost_matrix = squared_l2_matrix_np(X,Y)\n",
    "    log_X_prob = - np.log(X.shape[0])\n",
    "    log_Y_prob = - np.log(Y.shape[0])\n",
    "    psi_Y = np.zeros(Y.shape[0])\n",
    "    psi_X = np.zeros(X.shape[0])\n",
    "    for l in range(num_steps-1):\n",
    "        psi_X, psi_Y = sinkhorn_step_np(psi_X, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon)\n",
    "    psi_X, psi_Y, diff = sinkhorn_step_np(psi_X, psi_Y, log_X_prob, log_Y_prob, \\\n",
    "                                       cost_matrix, epsilon, return_diff = True)\n",
    "    #return psi_X, psi_Y, diff\n",
    "    log_pi = log_coupling_np(psi_X, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon)\n",
    "    pi = np.exp(log_pi)\n",
    "    loss = np.sum(pi*(cost_matrix+epsilon*log_pi)) - epsilon*(log_X_prob + log_Y_prob)\n",
    "    #X and Y are uniform, so entropy = -log(probability)\n",
    "    return loss, diff\n",
    "\n",
    "def sinkhorn_dual_potentials_np(X, Y, epsilon, num_steps, psi_Y = None):\n",
    "    cost_matrix = squared_l2_matrix_np(X,Y)\n",
    "    log_X_prob = - np.log(X.shape[0])\n",
    "    log_Y_prob = - np.log(Y.shape[0])\n",
    "    if psi_Y is None:\n",
    "        psi_Y = np.zeros(Y.shape[0])\n",
    "        psi_X = np.zeros(X.shape[0])\n",
    "    for l in range(num_steps-1):\n",
    "        psi_X, psi_Y = sinkhorn_step_np(psi_X, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon)\n",
    "    psi_X, psi_Y, diff = sinkhorn_step_np(psi_X, psi_Y, log_X_prob, log_Y_prob, \\\n",
    "                                       cost_matrix, epsilon, return_diff = True)\n",
    "    #return psi_X, psi_Y, diff\n",
    "    #X and Y are uniform, so entropy = -log(probability)\n",
    "    return psi_X, psi_Y, diff\n",
    "\n",
    "def sinkhorn_dual_potentials_symm_np(X, epsilon, num_steps):\n",
    "    cost_matrix = squared_l2_matrix_np(X,X)\n",
    "    log_X_prob = - np.log(X.shape[0])\n",
    "    psi_X = np.zeros(X.shape[0])\n",
    "    for l in range(num_steps-1):\n",
    "        psi_X = sinkhorn_step_symm_np(psi_X, log_X_prob, cost_matrix, epsilon)\n",
    "    psi_X, diff = sinkhorn_step_symm_np(psi_X, log_X_prob, cost_matrix, epsilon, return_diff = True)\n",
    "    #return psi_X, psi_Y, diff\n",
    "    #X and Y are uniform, so entropy = -log(probability)\n",
    "    return psi_X, diff\n",
    "\n",
    "def K_tild(u,v,C,N,M,epsilon):\n",
    "    C_tild = C - tf.transpose(tf.reshape(tf.tile(u[:,0],[M]),[M,N])) - tf.reshape(tf.tile(v[:,0],[N]),[N,M])\n",
    "    K_tild = tf.exp(-C_tild/epsilon)\n",
    "    return K_tild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_l2_matrix(X, Y):\n",
    "    squared_l2_X = tf.reduce_sum(tf.square(X), axis = 1, keepdims=True)\n",
    "    squared_l2_Y = tf.reduce_sum(tf.square(Y), axis = 1, keepdims=True)\n",
    "    XY = tf.matmul(X,tf.transpose(Y))\n",
    "    return squared_l2_X + tf.transpose(squared_l2_Y) - 2 * XY\n",
    "\n",
    "def log_coupling(psi_X, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon):\n",
    "    C_tild = cost_matrix - tf.expand_dims(psi_X, axis=1) - tf.expand_dims(psi_Y, axis=0)\n",
    "    return -C_tild/epsilon + log_X_prob + log_Y_prob\n",
    "\n",
    "# def sinkhorn_step(psi_X, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon, return_diff = False):\n",
    "#     psi_X_upd = epsilon * (log_X_prob - \\\n",
    "#                        tf.reduce_logsumexp((tf.expand_dims(psi_Y, axis=0) - cost_matrix) / epsilon, axis = 1))\n",
    "#     psi_Y_upd = epsilon * (log_Y_prob - \\\n",
    "#                        tf.reduce_logsumexp((tf.expand_dims(psi_X_upd, axis=1) - cost_matrix)/epsilon,axis = 0))\n",
    "#     if return_diff:\n",
    "#         diff = tf.norm(psi_X_upd - psi_X) + tf.norm(psi_Y_upd - psi_Y)\n",
    "#     psi_X = psi_X_upd\n",
    "#     psi_Y = psi_Y_upd\n",
    "#     if return_diff:\n",
    "#         return psi_X, psi_Y, diff\n",
    "#     return psi_X, psi_Y\n",
    "\n",
    "# def sinkhorn_loss(X, Y, epsilon, num_steps, return_diff = False, return_diff_only = False):\n",
    "#     cost_matrix = squared_l2_matrix(X,Y)\n",
    "#     log_X_prob = - tf.math.log(tf.cast(tf.shape(X)[0], tf.float32))\n",
    "#     log_Y_prob = - tf.math.log(tf.cast(tf.shape(Y)[0], tf.float32))\n",
    "#     psi_Y = tf.zeros([tf.shape(Y)[0]])\n",
    "#     for l in range(num_steps-1):\n",
    "#         psi_X, psi_Y = sinkhorn_step(None, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon)\n",
    "#     psi_X, psi_Y, diff = sinkhorn_step(psi_X, psi_Y, log_X_prob, log_Y_prob, \\\n",
    "#                                        cost_matrix, epsilon, return_diff = True)\n",
    "#     #return psi_X, psi_Y, diff\n",
    "#     log_pi = log_coupling(psi_X, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon)\n",
    "#     pi = tf.exp(log_pi)\n",
    "#     loss = tf.reduce_sum(pi*(cost_matrix+epsilon*log_pi)) - epsilon*(log_X_prob + log_Y_prob)\n",
    "#     #X and Y are uniform, so entropy = -log(probability)\n",
    "#     if return_diff_only:\n",
    "#         return diff\n",
    "#     if return_diff:\n",
    "#         return loss, diff\n",
    "#     return loss\n",
    "\n",
    "class dual_potentials:\n",
    "    def __init__(self):\n",
    "        self.psi_X = None\n",
    "        self.psi_Y = None\n",
    "        self.diff = 0\n",
    "\n",
    "class dual_potentials_symm:\n",
    "    def __init__(self):\n",
    "        self.psi_X = np.zeros(BATCH_SIZE)\n",
    "        self.diff = 0\n",
    "\n",
    "def sinkhorn_loss_from_potentials(X, Y, epsilon, dual_vars):\n",
    "    cost_matrix = squared_l2_matrix(X,Y)\n",
    "    log_X_prob = - tf.math.log(tf.cast(tf.shape(X)[0], tf.float32))\n",
    "    log_Y_prob = - tf.math.log(tf.cast(tf.shape(Y)[0], tf.float32))\n",
    "    \n",
    "    log_pi = log_coupling(dual_vars.psi_X, dual_vars.psi_Y, \\\n",
    "                          log_X_prob, log_Y_prob, cost_matrix, epsilon)\n",
    "    log_pi = tf.minimum(log_pi, log_X_prob)\n",
    "    pi = tf.exp(log_pi)\n",
    "    loss = tf.reduce_sum(pi*(cost_matrix+epsilon*log_pi))\n",
    "    #X and Y are uniform, so entropy = -log(probability)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_train_gen():\n",
    "    np.random.seed(1)\n",
    "    full_dataset = np.random.randn(SAMPLE_SIZE,DATA_DIM) / np.sqrt(COVARIANCE_SCALE) \n",
    "    i = 0\n",
    "    offset = 0\n",
    "    while True:\n",
    "        dataset = full_dataset[i*BATCH_SIZE+offset:(i+1)*BATCH_SIZE+offset,:]\n",
    "        if (i+1)*BATCH_SIZE+offset > SAMPLE_SIZE: \n",
    "            offset = (i+1)*BATCH_SIZE+offset - SAMPLE_SIZE\n",
    "            np.random.shuffle(full_dataset)\n",
    "            dataset = np.concatenate([dataset,full_dataset[:offset,:]], axis = 0)\n",
    "            i = -1 \n",
    "        i+=1\n",
    "        yield dataset\n",
    "data_gen = inf_train_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator:\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 10,720\n",
      "Trainable params: 10,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#class WGAN:\n",
    " #   def __init__()self\n",
    "latent_sample = keras.Input(shape=(LATENT_DIM,))\n",
    "hidden_layer = latent_sample\n",
    "for i in range(3):\n",
    "    hidden_layer = layers.Dense(DIM, activation=\"relu\", kernel_initializer=weight_initializer,\n",
    "        bias_initializer=bias_initializer)(hidden_layer)\n",
    "output = layers.Dense(DATA_DIM, kernel_initializer=weight_initializer,\n",
    "                      bias_initializer=bias_initializer)(hidden_layer)\n",
    "generator = keras.Model(inputs=latent_sample, outputs = output, name=\"generator\")\n",
    "print('Generator:')\n",
    "print(generator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_vars = dual_potentials()\n",
    "dual_vars_x = dual_potentials()#true\n",
    "dual_vars_y = dual_potentials()#pred\n",
    "#MODE = 'loss'\n",
    "epsilon = LAMBDA\n",
    "if MODE == 'divergence':\n",
    "    model_loss = lambda x, y: sinkhorn_loss_from_potentials(x, y, epsilon, dual_vars) - 0.5*\\\n",
    "        (sinkhorn_loss_from_potentials(x, x, epsilon, dual_vars_x))\n",
    "else:\n",
    "    model_loss = lambda x, y: sinkhorn_loss(x, y, epsilon, CRITIC_ITERS, dual_vars)\n",
    "\n",
    "if MODE == 'divergence':\n",
    "    args = [epsilon, CRITIC_ITERS, True, True]\n",
    "    sinkhorn_diff = lambda x, y: (sinkhorn_loss(x, y, *args)\\\n",
    "        +sinkhorn_loss(x, x, *args)+sinkhorn_loss(y, y, *args))/3\n",
    "else:\n",
    "    sinkhorn_diff = lambda x, y: sinkhorn_loss(x, y, epsilon, CRITIC_ITERS)\n",
    "\n",
    "metric = lambda x, y: tf.norm(tfp.stats.covariance(x) - tfp.stats.covariance(y))\n",
    "\n",
    "generator.compile(optimizer=\"RMSprop\", loss=model_loss, metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch = next(data_gen)\n",
    "x_batch = np.random.normal(size=(y_batch.shape[0], LATENT_DIM))\n",
    "y_sample = np.array(generator.predict(x_batch, batch_size=BATCH_SIZE))\n",
    "res_train = {'loss':{}, 'cov_diff' : {}}\n",
    "res_test = {'loss':{}, 'cov_diff' : {}, 'sample':{}, 'sink_eps':{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Iteration  0\n",
      "Training: loss 0.827548086643219, covariance difference 13.74793815612793\n",
      "True\n",
      "True\n",
      "Iteration  1\n",
      "Training: loss 3831.65478515625, covariance difference 18.534156799316406\n",
      "True\n",
      "True\n",
      "Iteration  2\n",
      "Training: loss 3215.572509765625, covariance difference 26.967411041259766\n",
      "True\n",
      "True\n",
      "Iteration  3\n",
      "Training: loss 2261.848388671875, covariance difference 33.0068244934082\n",
      "True\n",
      "True\n",
      "Iteration  4\n",
      "Training: loss 1352.07177734375, covariance difference 37.229461669921875\n",
      "True\n",
      "True\n",
      "Iteration  5\n",
      "Training: loss 1082.1693115234375, covariance difference 55.39617156982422\n",
      "True\n",
      "True\n",
      "Iteration  6\n",
      "Training: loss 1275.5467529296875, covariance difference 52.75816345214844\n",
      "True\n",
      "True\n",
      "Iteration  7\n",
      "Training: loss 973.2342529296875, covariance difference 58.38312530517578\n",
      "True\n",
      "True\n",
      "Iteration  8\n",
      "Training: loss 1088.186767578125, covariance difference 67.35969543457031\n",
      "True\n",
      "True\n",
      "Iteration  9\n",
      "Training: loss 509.3439636230469, covariance difference 77.8211441040039\n",
      "True\n",
      "True\n",
      "Iteration  10\n",
      "Training: loss 480.5274353027344, covariance difference 79.17304992675781\n",
      "True\n",
      "True\n",
      "Iteration  11\n",
      "Training: loss 681.4252319335938, covariance difference 80.05889892578125\n",
      "True\n",
      "True\n",
      "Iteration  12\n",
      "Training: loss 359.1676330566406, covariance difference 83.84132385253906\n",
      "True\n",
      "True\n",
      "Iteration  13\n",
      "Training: loss 487.7419738769531, covariance difference 96.84121704101562\n",
      "True\n",
      "True\n",
      "Iteration  14\n",
      "Training: loss 247.01278686523438, covariance difference 92.68525695800781\n",
      "True\n",
      "True\n",
      "Iteration  15\n",
      "Training: loss 372.0865173339844, covariance difference 116.8037338256836\n",
      "True\n",
      "True\n",
      "Iteration  16\n",
      "Training: loss 381.40399169921875, covariance difference 123.65027618408203\n",
      "True\n",
      "True\n",
      "Iteration  17\n",
      "Training: loss 398.0151672363281, covariance difference 101.033203125\n",
      "True\n",
      "True\n",
      "Iteration  18\n",
      "Training: loss 252.426025390625, covariance difference 107.84769439697266\n",
      "True\n",
      "True\n",
      "Iteration  19\n",
      "Training: loss 143.7703857421875, covariance difference 112.59325408935547\n",
      "True\n",
      "True\n",
      "Iteration  20\n",
      "Training: loss 431.8470153808594, covariance difference 108.02557373046875\n",
      "True\n",
      "True\n",
      "Iteration  21\n",
      "Training: loss 86.96975708007812, covariance difference 103.15961456298828\n",
      "True\n",
      "True\n",
      "Iteration  22\n",
      "Training: loss 232.00311279296875, covariance difference 134.7064971923828\n",
      "True\n",
      "True\n",
      "Iteration  23\n",
      "Training: loss 241.50686645507812, covariance difference 95.74512481689453\n",
      "True\n",
      "True\n",
      "Iteration  24\n",
      "Training: loss 471.92877197265625, covariance difference 104.18189239501953\n",
      "True\n",
      "True\n",
      "Iteration  25\n",
      "Training: loss 223.29861450195312, covariance difference 138.8464813232422\n",
      "True\n",
      "True\n",
      "Iteration  26\n",
      "Training: loss 232.0550994873047, covariance difference 133.81216430664062\n",
      "True\n",
      "True\n",
      "Iteration  27\n",
      "Training: loss 488.375244140625, covariance difference 137.48362731933594\n",
      "True\n",
      "True\n",
      "Iteration  28\n",
      "Training: loss 152.7808837890625, covariance difference 131.79737854003906\n",
      "True\n",
      "True\n",
      "Iteration  29\n",
      "Training: loss 239.79933166503906, covariance difference 127.93000030517578\n",
      "True\n",
      "True\n",
      "Iteration  30\n",
      "Training: loss 173.17507934570312, covariance difference 146.11793518066406\n",
      "True\n",
      "True\n",
      "Iteration  31\n",
      "Training: loss 30.421037673950195, covariance difference 142.70870971679688\n",
      "True\n",
      "True\n",
      "Iteration  32\n",
      "Training: loss 33.38964080810547, covariance difference 135.53253173828125\n",
      "True\n",
      "True\n",
      "Iteration  33\n",
      "Training: loss 146.96096801757812, covariance difference 172.8373565673828\n",
      "True\n",
      "True\n",
      "Iteration  34\n",
      "Training: loss 98.64047241210938, covariance difference 165.2506866455078\n",
      "True\n",
      "True\n",
      "Iteration  35\n",
      "Training: loss 268.318359375, covariance difference 217.8999481201172\n",
      "True\n",
      "True\n",
      "Iteration  36\n",
      "Training: loss 91.52155303955078, covariance difference 187.45394897460938\n",
      "True\n",
      "True\n",
      "Iteration  37\n",
      "Training: loss 163.1763916015625, covariance difference 143.98814392089844\n",
      "True\n",
      "True\n",
      "Iteration  38\n",
      "Training: loss 108.1398696899414, covariance difference 174.29379272460938\n",
      "True\n",
      "True\n",
      "Iteration  39\n",
      "Training: loss 185.7128143310547, covariance difference 216.7642364501953\n",
      "True\n",
      "True\n",
      "Iteration  40\n",
      "Training: loss 72.92081451416016, covariance difference 201.5706329345703\n",
      "True\n",
      "True\n",
      "Iteration  41\n",
      "Training: loss 61.885780334472656, covariance difference 169.64268493652344\n",
      "True\n",
      "True\n",
      "Iteration  42\n",
      "Training: loss 90.71147918701172, covariance difference 199.86764526367188\n",
      "True\n",
      "True\n",
      "Iteration  43\n",
      "Training: loss 261.4486999511719, covariance difference 176.87472534179688\n",
      "True\n",
      "True\n",
      "Iteration  44\n",
      "Training: loss 65.30792999267578, covariance difference 216.559814453125\n",
      "True\n",
      "True\n",
      "Iteration  45\n",
      "Training: loss 81.66761779785156, covariance difference 193.0504913330078\n",
      "True\n",
      "True\n",
      "Iteration  46\n",
      "Training: loss 111.59690856933594, covariance difference 219.50289916992188\n",
      "True\n",
      "True\n",
      "Iteration  47\n",
      "Training: loss 0.8269141316413879, covariance difference 204.73690795898438\n",
      "True\n",
      "True\n",
      "Iteration  48\n",
      "Training: loss 49.83845520019531, covariance difference 204.47581481933594\n",
      "True\n",
      "True\n",
      "Iteration  49\n",
      "Training: loss 102.73072052001953, covariance difference 199.97300720214844\n",
      "True\n",
      "True\n",
      "Iteration  50\n",
      "Training: loss 156.55618286132812, covariance difference 215.30157470703125\n",
      "True\n",
      "True\n",
      "Iteration  51\n",
      "Training: loss 142.98670959472656, covariance difference 204.6693572998047\n",
      "True\n",
      "True\n",
      "Iteration  52\n",
      "Training: loss 72.71208953857422, covariance difference 184.86880493164062\n",
      "True\n",
      "True\n",
      "Iteration  53\n",
      "Training: loss 20.985803604125977, covariance difference 289.8707275390625\n",
      "True\n",
      "True\n",
      "Iteration  54\n",
      "Training: loss 74.76918029785156, covariance difference 237.6844482421875\n",
      "True\n",
      "True\n",
      "Iteration  55\n",
      "Training: loss 1.5275533199310303, covariance difference 254.45819091796875\n",
      "True\n",
      "True\n",
      "Iteration  56\n",
      "Training: loss 0.8268798589706421, covariance difference 268.5743103027344\n",
      "True\n",
      "True\n",
      "Iteration  57\n",
      "Training: loss 0.8268800973892212, covariance difference 246.32730102539062\n",
      "True\n",
      "True\n",
      "Iteration  58\n",
      "Training: loss 38.375274658203125, covariance difference 260.92535400390625\n",
      "True\n",
      "True\n",
      "Iteration  59\n",
      "Training: loss 0.8268795609474182, covariance difference 238.6163330078125\n",
      "True\n",
      "True\n",
      "Iteration  60\n",
      "Training: loss 0.8268800973892212, covariance difference 220.47006225585938\n",
      "True\n",
      "True\n",
      "Iteration  61\n",
      "Training: loss 18.884586334228516, covariance difference 265.90411376953125\n",
      "True\n",
      "True\n",
      "Iteration  62\n",
      "Training: loss 1.5398542881011963, covariance difference 323.0764465332031\n",
      "True\n",
      "True\n",
      "Iteration  63\n",
      "Training: loss 0.8269504308700562, covariance difference 249.39610290527344\n",
      "True\n",
      "True\n",
      "Iteration  64\n",
      "Training: loss 89.9699478149414, covariance difference 374.07965087890625\n",
      "True\n",
      "True\n",
      "Iteration  65\n",
      "Training: loss 0.8268803954124451, covariance difference 278.76165771484375\n",
      "True\n",
      "True\n",
      "Iteration  66\n",
      "Training: loss 40.24569320678711, covariance difference 280.74334716796875\n",
      "True\n",
      "True\n",
      "Iteration  67\n",
      "Training: loss 0.8268798589706421, covariance difference 292.7192687988281\n",
      "True\n",
      "True\n",
      "Iteration  68\n",
      "Training: loss 0.8268797397613525, covariance difference 284.9326171875\n",
      "True\n",
      "True\n",
      "Iteration  69\n",
      "Training: loss 61.47362518310547, covariance difference 235.0043487548828\n",
      "True\n",
      "True\n",
      "Iteration  70\n",
      "Training: loss 0.8268800973892212, covariance difference 303.7253112792969\n",
      "True\n",
      "True\n",
      "Iteration  71\n",
      "Training: loss 0.8268795609474182, covariance difference 308.7554016113281\n",
      "True\n",
      "True\n",
      "Iteration  72\n",
      "Training: loss 38.384796142578125, covariance difference 291.9366760253906\n",
      "True\n",
      "True\n",
      "Iteration  73\n",
      "Training: loss 68.64722442626953, covariance difference 393.0796203613281\n",
      "True\n",
      "True\n",
      "Iteration  74\n",
      "Training: loss 99.48287200927734, covariance difference 284.81103515625\n",
      "True\n",
      "True\n",
      "Iteration  75\n",
      "Training: loss 0.8268800973892212, covariance difference 291.13653564453125\n",
      "True\n",
      "True\n",
      "Iteration  76\n",
      "Training: loss 84.56981658935547, covariance difference 349.281982421875\n",
      "True\n",
      "True\n",
      "Iteration  77\n",
      "Training: loss 0.8268798589706421, covariance difference 363.8171691894531\n",
      "True\n",
      "True\n",
      "Iteration  78\n",
      "Training: loss 0.8268808126449585, covariance difference 380.1046447753906\n",
      "True\n",
      "True\n",
      "Iteration  79\n",
      "Training: loss 0.8268794417381287, covariance difference 380.3515625\n",
      "True\n",
      "True\n",
      "Iteration  80\n",
      "Training: loss 104.20044708251953, covariance difference 359.81207275390625\n",
      "True\n",
      "True\n",
      "Iteration  81\n",
      "Training: loss 48.583839416503906, covariance difference 386.0583190917969\n",
      "True\n",
      "True\n",
      "Iteration  82\n",
      "Training: loss 0.826879620552063, covariance difference 381.93817138671875\n",
      "True\n",
      "True\n",
      "Iteration  83\n",
      "Training: loss 64.14618682861328, covariance difference 398.73114013671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Iteration  84\n",
      "Training: loss 0.826879620552063, covariance difference 383.71917724609375\n",
      "True\n",
      "True\n",
      "Iteration  85\n",
      "Training: loss 0.8268800973892212, covariance difference 312.02490234375\n",
      "True\n",
      "True\n",
      "Iteration  86\n",
      "Training: loss 0.8268795609474182, covariance difference 348.30877685546875\n",
      "True\n",
      "True\n",
      "Iteration  87\n",
      "Training: loss 109.18159484863281, covariance difference 363.69580078125\n",
      "True\n",
      "True\n",
      "Iteration  88\n",
      "Training: loss 0.8268797397613525, covariance difference 311.3428039550781\n",
      "True\n",
      "True\n",
      "Iteration  89\n",
      "Training: loss 0.8268803358078003, covariance difference 394.6433410644531\n",
      "True\n",
      "True\n",
      "Iteration  90\n",
      "Training: loss 24.17708969116211, covariance difference 458.63970947265625\n",
      "True\n",
      "True\n",
      "Iteration  91\n",
      "Training: loss 0.8268795013427734, covariance difference 459.36395263671875\n",
      "True\n",
      "True\n",
      "Iteration  92\n",
      "Training: loss 0.8268795013427734, covariance difference 413.78857421875\n",
      "True\n",
      "True\n",
      "Iteration  93\n",
      "Training: loss 0.8268795609474182, covariance difference 420.3114929199219\n",
      "True\n",
      "True\n",
      "Iteration  94\n",
      "Training: loss 0.8268798589706421, covariance difference 431.4170227050781\n",
      "True\n",
      "True\n",
      "Iteration  95\n",
      "Training: loss 0.8268799781799316, covariance difference 364.5068359375\n",
      "True\n",
      "True\n",
      "Iteration  96\n",
      "Training: loss 0.8268799781799316, covariance difference 412.6764831542969\n",
      "True\n",
      "True\n",
      "Iteration  97\n",
      "Training: loss 0.826879620552063, covariance difference 412.1560363769531\n",
      "True\n",
      "True\n",
      "Iteration  98\n",
      "Training: loss 0.8268797397613525, covariance difference 352.0660095214844\n",
      "True\n",
      "True\n",
      "Iteration  99\n",
      "Training: loss 0.8268797397613525, covariance difference 378.27911376953125\n",
      "True\n",
      "True\n",
      "Iteration  100\n",
      "Training: loss 0.8268798589706421, covariance difference 377.7166748046875\n",
      "True\n",
      "True\n",
      "Iteration  101\n",
      "Training: loss 0.8268814086914062, covariance difference 414.4952392578125\n",
      "True\n",
      "True\n",
      "Iteration  102\n",
      "Training: loss 0.8268805146217346, covariance difference 412.5279846191406\n",
      "True\n",
      "True\n",
      "Iteration  103\n",
      "Training: loss 0.826880931854248, covariance difference 402.4183654785156\n",
      "True\n",
      "True\n",
      "Iteration  104\n",
      "Training: loss 0.8268796801567078, covariance difference 442.4524230957031\n",
      "True\n",
      "True\n",
      "Iteration  105\n",
      "Training: loss 0.8268795013427734, covariance difference 421.2084045410156\n",
      "True\n",
      "True\n",
      "Iteration  106\n",
      "Training: loss 0.8268795013427734, covariance difference 453.2526550292969\n",
      "True\n",
      "True\n",
      "Iteration  107\n",
      "Training: loss 0.8268797993659973, covariance difference 372.763427734375\n",
      "True\n",
      "True\n",
      "Iteration  108\n",
      "Training: loss 0.8268798589706421, covariance difference 384.4136657714844\n",
      "True\n",
      "True\n",
      "Iteration  109\n",
      "Training: loss 0.8268803954124451, covariance difference 417.6403503417969\n",
      "True\n",
      "True\n",
      "Iteration  110\n",
      "Training: loss 0.826879620552063, covariance difference 411.285888671875\n",
      "True\n",
      "True\n",
      "Iteration  111\n",
      "Training: loss 0.8268800973892212, covariance difference 388.2352294921875\n",
      "True\n",
      "True\n",
      "Iteration  112\n",
      "Training: loss 0.8268797993659973, covariance difference 483.12127685546875\n",
      "True\n",
      "True\n",
      "Iteration  113\n",
      "Training: loss 12.496397018432617, covariance difference 445.17608642578125\n",
      "True\n",
      "True\n",
      "Iteration  114\n",
      "Training: loss 0.8268828392028809, covariance difference 496.0596618652344\n",
      "True\n",
      "True\n",
      "Iteration  115\n",
      "Training: loss 0.8268799781799316, covariance difference 409.8889465332031\n",
      "True\n",
      "True\n",
      "Iteration  116\n",
      "Training: loss 0.8268797993659973, covariance difference 409.85223388671875\n",
      "True\n",
      "True\n",
      "Iteration  117\n",
      "Training: loss 0.8268799185752869, covariance difference 421.03851318359375\n",
      "True\n",
      "True\n",
      "Iteration  118\n",
      "Training: loss 0.8268809914588928, covariance difference 486.6860656738281\n",
      "True\n",
      "True\n",
      "Iteration  119\n",
      "Training: loss 0.8268799781799316, covariance difference 461.9254150390625\n",
      "True\n",
      "True\n",
      "Iteration  120\n",
      "Training: loss 0.8268800973892212, covariance difference 470.7253112792969\n",
      "True\n",
      "True\n",
      "Iteration  121\n",
      "Training: loss 0.8268800973892212, covariance difference 423.5074462890625\n",
      "True\n",
      "True\n",
      "Iteration  122\n",
      "Training: loss 0.8268809914588928, covariance difference 509.891845703125\n",
      "True\n",
      "True\n",
      "Iteration  123\n",
      "Training: loss 0.8268798589706421, covariance difference 468.2177734375\n",
      "True\n",
      "True\n",
      "Iteration  124\n",
      "Training: loss 58.35771179199219, covariance difference 385.6860656738281\n",
      "True\n",
      "True\n",
      "Iteration  125\n",
      "Training: loss 0.8268815279006958, covariance difference 602.5147705078125\n",
      "True\n",
      "True\n",
      "Iteration  126\n",
      "Training: loss 0.8268799781799316, covariance difference 537.2628173828125\n",
      "True\n",
      "True\n",
      "Iteration  127\n",
      "Training: loss 0.826880156993866, covariance difference 461.3265075683594\n",
      "True\n",
      "True\n",
      "Iteration  128\n",
      "Training: loss 0.8268803954124451, covariance difference 522.205810546875\n",
      "True\n",
      "True\n",
      "Iteration  129\n",
      "Training: loss 0.8268798589706421, covariance difference 592.738037109375\n",
      "True\n",
      "True\n",
      "Iteration  130\n",
      "Training: loss 0.8268798589706421, covariance difference 493.1603088378906\n",
      "True\n",
      "True\n",
      "Iteration  131\n",
      "Training: loss 101.28990936279297, covariance difference 511.4773864746094\n",
      "True\n",
      "True\n",
      "Iteration  132\n",
      "Training: loss 0.8268802165985107, covariance difference 621.0004272460938\n",
      "True\n",
      "True\n",
      "Iteration  133\n",
      "Training: loss 0.8268802165985107, covariance difference 621.1738891601562\n",
      "True\n",
      "True\n",
      "Iteration  134\n",
      "Training: loss 0.8268799781799316, covariance difference 500.3823547363281\n",
      "True\n",
      "True\n",
      "Iteration  135\n",
      "Training: loss 0.826879620552063, covariance difference 483.134033203125\n",
      "True\n",
      "True\n",
      "Iteration  136\n",
      "Training: loss 0.8268798589706421, covariance difference 455.0573425292969\n",
      "True\n",
      "True\n",
      "Iteration  137\n",
      "Training: loss 0.8268799185752869, covariance difference 551.8731079101562\n",
      "True\n",
      "True\n",
      "Iteration  138\n",
      "Training: loss 0.8268810510635376, covariance difference 508.851318359375\n",
      "True\n",
      "True\n",
      "Iteration  139\n",
      "Training: loss 0.8268800377845764, covariance difference 462.3155822753906\n",
      "True\n",
      "True\n",
      "Iteration  140\n",
      "Training: loss 103.02761840820312, covariance difference 622.7593383789062\n",
      "True\n",
      "True\n",
      "Iteration  141\n",
      "Training: loss 0.826879620552063, covariance difference 490.7762756347656\n",
      "True\n",
      "True\n",
      "Iteration  142\n",
      "Training: loss 0.8268805742263794, covariance difference 415.00347900390625\n",
      "True\n",
      "True\n",
      "Iteration  143\n",
      "Training: loss 0.8268797993659973, covariance difference 560.8231811523438\n",
      "True\n",
      "True\n",
      "Iteration  144\n",
      "Training: loss 0.8268799781799316, covariance difference 501.4970703125\n",
      "True\n",
      "True\n",
      "Iteration  145\n",
      "Training: loss 0.8268795013427734, covariance difference 410.84686279296875\n",
      "True\n",
      "True\n",
      "Iteration  146\n",
      "Training: loss 0.826880156993866, covariance difference 525.6019287109375\n",
      "True\n",
      "True\n",
      "Iteration  147\n",
      "Training: loss 0.8268803358078003, covariance difference 483.3655700683594\n",
      "True\n",
      "True\n",
      "Iteration  148\n",
      "Training: loss 0.8268799781799316, covariance difference 538.7936401367188\n",
      "True\n",
      "True\n",
      "Iteration  149\n",
      "Training: loss 0.8268799185752869, covariance difference 522.7913208007812\n",
      "True\n",
      "True\n",
      "Iteration  150\n",
      "Training: loss 0.8268795013427734, covariance difference 487.3917541503906\n",
      "True\n",
      "True\n",
      "Iteration  151\n",
      "Training: loss 0.826879620552063, covariance difference 525.5422973632812\n",
      "True\n",
      "True\n",
      "Iteration  152\n",
      "Training: loss 0.8268793821334839, covariance difference 536.9328002929688\n",
      "True\n",
      "True\n",
      "Iteration  153\n",
      "Training: loss 0.8268799781799316, covariance difference 428.2312927246094\n",
      "True\n",
      "True\n",
      "Iteration  154\n",
      "Training: loss 0.8268802165985107, covariance difference 473.8151550292969\n",
      "True\n",
      "True\n",
      "Iteration  155\n",
      "Training: loss 0.8268797397613525, covariance difference 568.33349609375\n",
      "True\n",
      "True\n",
      "Iteration  156\n",
      "Training: loss 0.8268797397613525, covariance difference 466.1880798339844\n",
      "True\n",
      "True\n",
      "Iteration  157\n",
      "Training: loss 0.8268795013427734, covariance difference 520.6163940429688\n",
      "True\n",
      "True\n",
      "Iteration  158\n",
      "Training: loss 0.8268802762031555, covariance difference 407.217529296875\n",
      "True\n",
      "True\n",
      "Iteration  159\n",
      "Training: loss 0.8268793821334839, covariance difference 468.6134033203125\n",
      "True\n",
      "True\n",
      "Iteration  160\n",
      "Training: loss 0.826880156993866, covariance difference 505.67864990234375\n",
      "True\n",
      "True\n",
      "Iteration  161\n",
      "Training: loss 0.8268799781799316, covariance difference 560.995849609375\n",
      "True\n",
      "True\n",
      "Iteration  162\n",
      "Training: loss 0.8268795013427734, covariance difference 606.4091796875\n",
      "True\n",
      "True\n",
      "Iteration  163\n",
      "Training: loss 0.8268799781799316, covariance difference 450.90985107421875\n",
      "True\n",
      "True\n",
      "Iteration  164\n",
      "Training: loss 0.8268796801567078, covariance difference 477.47723388671875\n",
      "True\n",
      "True\n",
      "Iteration  165\n",
      "Training: loss 0.8268805146217346, covariance difference 537.9234008789062\n",
      "True\n",
      "True\n",
      "Iteration  166\n",
      "Training: loss 0.8268799781799316, covariance difference 497.4568786621094\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  167\n",
      "Training: loss 0.826880931854248, covariance difference 452.7852478027344\n",
      "True\n",
      "True\n",
      "Iteration  168\n",
      "Training: loss 0.8268794417381287, covariance difference 529.7664794921875\n",
      "True\n",
      "True\n",
      "Iteration  169\n",
      "Training: loss 0.8268791437149048, covariance difference 525.42919921875\n",
      "True\n",
      "True\n",
      "Iteration  170\n",
      "Training: loss 0.826879620552063, covariance difference 475.8092346191406\n",
      "True\n",
      "True\n",
      "Iteration  171\n",
      "Training: loss 0.8268797397613525, covariance difference 667.373046875\n",
      "True\n",
      "True\n",
      "Iteration  172\n",
      "Training: loss 0.8268798589706421, covariance difference 456.39447021484375\n",
      "True\n",
      "True\n",
      "Iteration  173\n",
      "Training: loss 0.8268795013427734, covariance difference 559.6903686523438\n",
      "True\n",
      "True\n",
      "Iteration  174\n",
      "Training: loss 0.826880156993866, covariance difference 525.8646240234375\n",
      "True\n",
      "True\n",
      "Iteration  175\n",
      "Training: loss 0.8268799185752869, covariance difference 538.6388549804688\n",
      "True\n",
      "True\n",
      "Iteration  176\n",
      "Training: loss 0.8268803954124451, covariance difference 436.82171630859375\n",
      "True\n",
      "True\n",
      "Iteration  177\n",
      "Training: loss 0.8268795013427734, covariance difference 472.7784118652344\n",
      "True\n",
      "True\n",
      "Iteration  178\n",
      "Training: loss 0.8268795609474182, covariance difference 455.57391357421875\n",
      "True\n",
      "True\n",
      "Iteration  179\n",
      "Training: loss 0.8268798589706421, covariance difference 506.7883605957031\n",
      "True\n",
      "True\n",
      "Iteration  180\n",
      "Training: loss 29.6240177154541, covariance difference 523.0068969726562\n",
      "True\n",
      "True\n",
      "Iteration  181\n",
      "Training: loss 0.8268797397613525, covariance difference 624.06982421875\n",
      "True\n",
      "True\n",
      "Iteration  182\n",
      "Training: loss 0.8268793821334839, covariance difference 602.297607421875\n",
      "True\n",
      "True\n",
      "Iteration  183\n",
      "Training: loss 0.8268796801567078, covariance difference 578.428466796875\n",
      "True\n",
      "True\n",
      "Iteration  184\n",
      "Training: loss 0.8268808126449585, covariance difference 617.7435913085938\n",
      "True\n",
      "True\n",
      "Iteration  185\n",
      "Training: loss 0.8268797397613525, covariance difference 618.0560913085938\n",
      "True\n",
      "True\n",
      "Iteration  186\n",
      "Training: loss 0.8268799185752869, covariance difference 569.0300903320312\n",
      "True\n",
      "True\n",
      "Iteration  187\n",
      "Training: loss 0.8268803358078003, covariance difference 541.6686401367188\n",
      "True\n",
      "True\n",
      "Iteration  188\n",
      "Training: loss 0.8268798589706421, covariance difference 684.8550415039062\n",
      "True\n",
      "True\n",
      "Iteration  189\n",
      "Training: loss 0.8268793821334839, covariance difference 643.9181518554688\n",
      "True\n",
      "True\n",
      "Iteration  190\n",
      "Training: loss 0.8268817663192749, covariance difference 710.7302856445312\n",
      "True\n",
      "True\n",
      "Iteration  191\n",
      "Training: loss 0.8268799781799316, covariance difference 698.6583862304688\n",
      "True\n",
      "True\n",
      "Iteration  192\n",
      "Training: loss 0.8268811702728271, covariance difference 675.3764038085938\n",
      "True\n",
      "True\n",
      "Iteration  193\n",
      "Training: loss 0.8268793821334839, covariance difference 572.3135986328125\n",
      "True\n",
      "True\n",
      "Iteration  194\n",
      "Training: loss 0.8268793821334839, covariance difference 622.2225341796875\n",
      "True\n",
      "True\n",
      "Iteration  195\n",
      "Training: loss 0.8268799781799316, covariance difference 610.9964599609375\n",
      "True\n",
      "True\n",
      "Iteration  196\n",
      "Training: loss 0.8268797397613525, covariance difference 621.7567749023438\n",
      "True\n",
      "True\n",
      "Iteration  197\n",
      "Training: loss 0.8268803358078003, covariance difference 757.5227661132812\n",
      "True\n",
      "True\n",
      "Iteration  198\n",
      "Training: loss 0.8268797397613525, covariance difference 697.89794921875\n",
      "True\n",
      "True\n",
      "Iteration  199\n",
      "Training: loss 0.8268799781799316, covariance difference 605.9763793945312\n",
      "True\n",
      "True\n",
      "Iteration  200\n",
      "Training: loss 0.826881468296051, covariance difference 515.30712890625\n",
      "True\n",
      "True\n",
      "Iteration  201\n",
      "Training: loss 0.8268798589706421, covariance difference 597.827392578125\n",
      "True\n",
      "True\n",
      "Iteration  202\n",
      "Training: loss 0.826879620552063, covariance difference 672.4466552734375\n",
      "True\n",
      "True\n",
      "Iteration  203\n",
      "Training: loss 0.8268797397613525, covariance difference 626.5413208007812\n",
      "True\n",
      "True\n",
      "Iteration  204\n",
      "Training: loss 0.8268795013427734, covariance difference 755.6829833984375\n",
      "True\n",
      "True\n",
      "Iteration  205\n",
      "Training: loss 0.8268798589706421, covariance difference 597.1411743164062\n",
      "True\n",
      "True\n",
      "Iteration  206\n",
      "Training: loss 0.8268797993659973, covariance difference 704.2159423828125\n",
      "True\n",
      "True\n",
      "Iteration  207\n",
      "Training: loss 0.8268796801567078, covariance difference 632.1822509765625\n",
      "True\n",
      "True\n",
      "Iteration  208\n",
      "Training: loss 0.8268808126449585, covariance difference 600.509521484375\n",
      "True\n",
      "True\n",
      "Iteration  209\n",
      "Training: loss 0.8268798589706421, covariance difference 531.0724487304688\n",
      "True\n",
      "True\n",
      "Iteration  210\n",
      "Training: loss 0.8268796801567078, covariance difference 723.7583618164062\n",
      "True\n",
      "True\n",
      "Iteration  211\n",
      "Training: loss 0.8268821239471436, covariance difference 549.0689086914062\n",
      "True\n",
      "True\n",
      "Iteration  212\n",
      "Training: loss 0.8268803954124451, covariance difference 662.1185913085938\n",
      "True\n",
      "True\n",
      "Iteration  213\n",
      "Training: loss 0.8268795609474182, covariance difference 663.0406494140625\n",
      "True\n",
      "True\n",
      "Iteration  214\n",
      "Training: loss 0.8268793225288391, covariance difference 631.3040771484375\n",
      "True\n",
      "True\n",
      "Iteration  215\n",
      "Training: loss 0.8268797397613525, covariance difference 714.2364501953125\n",
      "True\n",
      "True\n",
      "Iteration  216\n",
      "Training: loss 0.826880156993866, covariance difference 594.79248046875\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a9910886cbc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdual_vars_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpsi_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdual_vars_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpsi_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdual_vars_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpsi_X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mpsi_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual_vars_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msinkhorn_dual_potentials_symm_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCRITIC_ITERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdual_vars_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpsi_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdual_vars_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpsi_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdual_vars_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpsi_X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-4a840b73524c>\u001b[0m in \u001b[0;36msinkhorn_dual_potentials_symm_np\u001b[0;34m(X, epsilon, num_steps)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mpsi_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msinkhorn_step_symm_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_X_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mpsi_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msinkhorn_step_symm_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_X_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;31m#return psi_X, psi_Y, diff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m#X and Y are uniform, so entropy = -log(probability)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-4a840b73524c>\u001b[0m in \u001b[0;36msinkhorn_step_symm_np\u001b[0;34m(psi_X, log_X_prob, cost_matrix, epsilon, return_diff)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msinkhorn_step_symm_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_X_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     psi_X_upd = (psi_X - epsilon * logsumexp(log_X_prob+(np.expand_dims(psi_X, axis=0) - \n\u001b[0;32m---> 34\u001b[0;31m                                                          cost_matrix) / epsilon, axis = 1))/2\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_diff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi_X_upd\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpsi_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-05b623d39f15>\u001b[0m in \u001b[0;36mlogsumexp\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#    val = np.nan_to_num(val, -5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     idx = np.where(val!=val)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/anaconda3/lib/python3.6/site-packages/scipy/special/_logsumexp.py\u001b[0m in \u001b[0;36mlogsumexp\u001b[0;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# suppress warnings about log of zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(ITERS):\n",
    "    psi_X, psi_Y, dual_vars.diff = sinkhorn_dual_potentials_np(y_batch, y_sample, epsilon, CRITIC_ITERS)\n",
    "    dual_vars.psi_X = tf.cast(psi_X, 'float32')\n",
    "    dual_vars.psi_Y = tf.cast(psi_Y, 'float32')\n",
    "    \n",
    "    #while dual_vars.diff > .1:\n",
    "    #    psi_X, psi_Y, dual_vars.diff = sinkhorn_dual_potentials_np(y_batch, y_sample, epsilon, CRITIC_ITERS, psi_Y)\n",
    "    #    dual_vars.psi_X = tf.cast(psi_X, 'float32')\n",
    "    #    dual_vars.psi_Y = tf.cast(psi_Y, 'float32')\n",
    "    print(np.isfinite(psi_X).all())\n",
    "    print(np.isfinite(psi_Y).all())\n",
    "        \n",
    "    psi_X, dual_vars_x.diff = sinkhorn_dual_potentials_symm_np(y_batch, epsilon, CRITIC_ITERS)\n",
    "    dual_vars_x.psi_X = tf.cast(psi_X, 'float32')\n",
    "    dual_vars_x.psi_Y = dual_vars_x.psi_X\n",
    "    psi_X, dual_vars_y.diff = sinkhorn_dual_potentials_symm_np(y_sample, epsilon, CRITIC_ITERS)\n",
    "    dual_vars_x.psi_X = tf.cast(psi_X, 'float32')\n",
    "    dual_vars_y.psi_Y = dual_vars_y.psi_X\n",
    "    out = generator.train_on_batch(x = x_batch,y = y_batch)\n",
    "    res_train['loss'][i] = out[0]\n",
    "    res_train['cov_diff'][i] = out[1]\n",
    "    y_batch = next(data_gen)\n",
    "    x_batch = np.random.normal(size=(y_batch.shape[0], LATENT_DIM))\n",
    "    #validate on the next data\n",
    "    y_sample = np.array(generator.predict(x_batch, batch_size=BATCH_SIZE))\n",
    "    #res_test['loss'][i], res_test['sink_eps'][i] = sinkhorn_loss_np(y_sample, y_batch, epsilon, CRITIC_ITERS)\n",
    "    #res_test['sample'][i] = y_sample\n",
    "    #res_test['cov_diff'][i] = np.linalg.norm(np.cov(y_sample) - np.cov(y_batch))\n",
    "    print('Iteration ', i)\n",
    "    print('Training: loss {}, covariance difference {}'.format(res_train['loss'][i], res_train['cov_diff'][i]))\n",
    "    #print('Validation: loss {}, covariance difference {}, sinkhorn epsilon {}'.format(\n",
    "    #    res_test['loss'][i], res_test['cov_diff'][i], res_test['sink_eps'][i]))\n",
    "    #if i % 10 == 0:\n",
    "    #    with open('logs.pkl', 'wb') as f:\n",
    "    #        pkl.dump([res_test, res_train], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_X_prob = - tf.math.log(tf.cast(tf.shape(hY)[0], tf.float32))\n",
    "log_Y_prob = - tf.math.log(tf.cast(tf.shape(Y)[0], tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pi = log_coupling(dual_vars.psi_X, dual_vars.psi_Y, \\\n",
    "                          log_X_prob, log_Y_prob, cost_matrix, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "522.78375"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(dual_vars.psi_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403.02386"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(cost_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1781.4918"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.array(log_pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 256), dtype=float32, numpy=\n",
       "array([[-113.68503 ,  -26.168352,  191.39458 , ...,  -63.165714,\n",
       "        -147.3079  , -204.64305 ],\n",
       "       [   9.470531,  118.72862 ,  322.0069  , ...,   49.36162 ,\n",
       "         -28.387571,  -88.39143 ],\n",
       "       [-127.35713 ,  -19.360134,  168.58821 , ...,  -66.24303 ,\n",
       "        -146.08418 , -216.43121 ],\n",
       "       ...,\n",
       "       [ -59.572117,    8.696758,  222.1756  , ...,  -49.007694,\n",
       "        -101.53757 , -165.60045 ],\n",
       "       [  40.280746,  124.44064 ,  362.07047 , ...,   66.759384,\n",
       "          30.082405,  -47.806503],\n",
       "       [  89.34366 ,  186.66008 ,  402.57306 , ...,  138.01866 ,\n",
       "          47.45031 ,  -19.337582]], dtype=float32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.27525902e-01, 7.00110734e-01, 9.10156250e-01, 3.64875168e-01,\n",
       "       1.11972511e-01, 1.82493269e-01, 5.31922817e-01, 3.63683224e-01,\n",
       "       6.51517451e-01, 8.64191175e-01, 1.82750538e-01, 8.04702520e-01,\n",
       "       3.71962599e-02, 6.25111222e-01, 9.53129113e-01, 9.26196456e-01,\n",
       "       9.72656250e-01, 1.75975874e-01, 9.53126967e-01, 7.31868148e-01,\n",
       "       9.88281250e-01, 1.18378878e-01, 5.51946104e-01, 9.72656250e-01,\n",
       "       6.56578004e-01, 7.34416366e-01, 9.80468750e-01, 3.03956419e-01,\n",
       "       7.08360314e-01, 7.61959612e-01, 8.09335828e-01, 9.88575637e-01,\n",
       "       6.86276019e-01, 6.93455637e-01, 7.68851042e-01, 6.22675538e-01,\n",
       "       4.43837434e-01, 8.68086159e-01, 2.36477908e-02, 5.00675082e-01,\n",
       "       9.26009297e-01, 9.76562738e-01, 8.24343801e-01, 7.04391956e-01,\n",
       "       9.25984621e-01, 2.92849123e-01, 4.63242471e-01, 9.37180161e-01,\n",
       "       9.64843750e-01, 4.57297564e-01, 9.57031250e-01, 8.18132997e-01,\n",
       "       7.30552733e-01, 9.37500000e-01, 2.01298624e-01, 8.35937500e-01,\n",
       "       2.58575737e-01, 3.98810714e-01, 7.81656623e-01, 6.42059922e-01,\n",
       "       8.29522848e-01, 4.93890256e-01, 9.49248552e-01, 6.83720589e-01,\n",
       "       3.55766088e-01, 4.02537942e-01, 8.24743390e-01, 6.56589508e-01,\n",
       "       7.82401502e-01, 4.47512478e-01, 3.75106275e-01, 9.51058984e-01,\n",
       "       3.95348251e-01, 8.98454010e-01, 3.26634616e-01, 6.48027539e-01,\n",
       "       3.09849381e-01, 9.72656250e-01, 5.45753360e-01, 4.71772283e-01,\n",
       "       5.22605240e-01, 9.64843750e-01, 8.04694057e-01, 9.92188215e-01,\n",
       "       9.42672133e-01, 7.70481765e-01, 3.98356974e-01, 9.53125417e-01,\n",
       "       9.29687500e-01, 7.75500894e-01, 9.25781250e-01, 8.67207289e-01,\n",
       "       8.55497062e-01, 9.80469286e-01, 8.16406250e-01, 2.00543985e-01,\n",
       "       9.72656250e-01, 9.48110297e-02, 9.06394124e-01, 5.59509754e-01,\n",
       "       2.10722417e-01, 6.16984427e-01, 4.91832584e-01, 5.83791554e-01,\n",
       "       1.00000000e+00, 9.64843750e-01, 4.01913583e-01, 9.76562977e-01,\n",
       "       9.07200575e-01, 7.90596545e-01, 9.84387696e-01, 6.57460532e-07,\n",
       "       7.65265584e-01, 9.57050681e-01, 7.99020231e-01, 9.88281250e-01,\n",
       "       6.61038816e-01, 1.26317233e-01, 8.51578951e-01, 1.77439541e-01,\n",
       "       9.18340862e-01, 8.08037817e-01, 7.96904564e-01, 4.66381013e-01,\n",
       "       7.22999156e-01, 5.04582524e-01, 9.76562500e-01, 5.59103847e-01,\n",
       "       9.76568401e-01, 6.51854873e-01, 3.99419516e-01, 5.62836349e-01,\n",
       "       5.13963878e-01, 3.18576217e-01, 9.18173194e-01, 6.67019606e-01,\n",
       "       8.59762013e-01, 9.37500000e-01, 3.63381058e-01, 4.47098106e-01,\n",
       "       2.49918014e-01, 2.62926906e-01, 1.48906291e-01, 9.25781608e-01,\n",
       "       6.60300612e-01, 5.84397912e-01, 4.22576517e-01, 9.76562500e-01,\n",
       "       9.41406250e-01, 3.20958257e-01, 7.07080126e-01, 9.57031250e-01,\n",
       "       4.28768873e-01, 1.25064924e-02, 1.10184096e-01, 5.23624830e-02,\n",
       "       9.88281250e-01, 5.70692480e-01, 4.54619914e-01, 1.00000000e+00,\n",
       "       7.03325570e-01, 7.95973837e-01, 9.53125775e-01, 9.72656250e-01,\n",
       "       6.60786852e-02, 3.10799241e-01, 5.12764812e-01, 9.72656250e-01,\n",
       "       5.26195168e-01, 1.95362853e-04, 9.60937500e-01, 9.17971253e-01,\n",
       "       6.65272593e-01, 9.84375000e-01, 7.51909256e-01, 1.37679949e-01,\n",
       "       8.28873739e-02, 5.28822899e-01, 8.86718988e-01, 1.89399689e-01,\n",
       "       3.12665030e-02, 1.00000000e+00, 9.96093750e-01, 2.02292278e-01,\n",
       "       4.21717912e-01, 9.72656250e-01, 4.22364056e-01, 6.17464960e-01,\n",
       "       8.35937560e-01, 9.38055634e-01, 5.59690952e-01, 6.59304798e-01,\n",
       "       9.69079852e-01, 9.88281250e-01, 9.37500000e-01, 2.21519709e-01,\n",
       "       6.88089967e-01, 4.64588016e-01, 1.00000000e+00, 5.19963145e-01,\n",
       "       6.58494949e-01, 2.23206758e-01, 9.49218750e-01, 8.82821500e-01,\n",
       "       7.88473845e-01, 6.10942781e-01, 1.45202667e-01, 4.32626978e-02,\n",
       "       9.76562977e-01, 1.51451498e-01, 6.30560040e-01, 9.10157800e-01,\n",
       "       3.56533945e-01, 6.72057092e-01, 1.29079849e-01, 5.53985476e-01,\n",
       "       9.72656250e-01, 8.71094406e-01, 4.02269274e-01, 4.77423549e-01,\n",
       "       9.41406369e-01, 6.76705241e-01, 1.19960770e-01, 2.23541722e-01,\n",
       "       7.10550010e-01, 8.66735518e-01, 6.95312500e-01, 9.96093750e-01,\n",
       "       8.74722064e-01, 9.96093750e-01, 9.88281250e-01, 2.51707081e-02,\n",
       "       4.47744161e-01, 3.09740633e-01, 6.90754533e-01, 7.99344301e-01,\n",
       "       4.58399147e-01, 7.24525332e-01, 9.96093750e-01, 2.77777344e-01,\n",
       "       1.00000000e+00, 7.42188573e-01, 4.91812408e-01, 7.00435400e-01,\n",
       "       4.25791353e-01, 1.00000000e+00, 9.37500000e-01, 9.72656250e-01,\n",
       "       4.23016906e-01, 4.06921029e-01, 9.88281250e-01, 7.62967348e-01,\n",
       "       8.28130782e-01, 6.22336268e-01, 4.22665864e-01, 1.49274692e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = tf.exp(np.minimum(log_pi, log_X_prob))\n",
    "np.sum(pi, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_vars.psi_X, dual_vars.psi_Y, dual_vars.diff = sinkhorn_dual_potentials_np(y_batch, y_sample, epsilon, CRITIC_ITERS)\n",
    "dual_vars_x.psi_X, dual_vars_x.diff = sinkhorn_dual_potentials_symm_np(y_batch, epsilon, CRITIC_ITERS)\n",
    "dual_vars_x.psi_Y = dual_vars_x.psi_X\n",
    "dual_vars_y.psi_X, dual_vars_y.diff = sinkhorn_dual_potentials_symm_np(y_sample, epsilon, CRITIC_ITERS)\n",
    "dual_vars_y.psi_Y = dual_vars_y.psi_X\n",
    "#out = generator.train_on_batch(x = x_batch,y = y_batch)\n",
    "#res_train['loss'][i] = out[0]\n",
    "#res_train['cov_diff'][i] = out[1]\n",
    "#y_batch = next(data_gen)\n",
    "#x_batch = np.random.normal(size=(y_batch.shape[0], LATENT_DIM))\n",
    "#validate on the next data\n",
    "#y_sample = np.array(generator.predict(x_batch, batch_size=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss(tf.cast(y_batch, 'float32'),tf.cast(y_sample, 'float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generator.train_on_batch(x = x_batch,y = y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(squared_l2_matrix_np(y_batch, y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.cast(y_sample, 'float32')\n",
    "X = tf.cast(y_batch, 'float32')\n",
    "dual_vars.psi_X, dual_vars.psi_Y, dual_vars.diff = sinkhorn_dual_potentials_np(y_batch, y_sample, epsilon, CRITIC_ITERS)\n",
    "dual_vars_x.psi_X, dual_vars_x.diff = sinkhorn_dual_potentials_symm_np(y_batch, epsilon, CRITIC_ITERS)\n",
    "dual_vars_x.psi_Y = dual_vars_x.psi_X\n",
    "dual_vars_y.psi_X, dual_vars_y.diff = sinkhorn_dual_potentials_symm_np(y_sample, epsilon, CRITIC_ITERS)\n",
    "dual_vars_y.psi_Y = dual_vars_y.psi_X\n",
    "model_loss(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generator.train_on_batch(x = x_batch,y = y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinkhorn_loss_from_potentials(Y, X, epsilon, CRITIC_ITERS, dual_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_matrix = squared_l2_matrix(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_X_prob = - tf.math.log(tf.cast(tf.shape(X)[0], tf.float32))\n",
    "log_Y_prob = - tf.math.log(tf.cast(tf.shape(Y)[0], tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pi = log_coupling(tf.cast(dual_vars.psi_X, 'float32'), tf.cast(dual_vars.psi_Y, 'float32'), log_X_prob, log_Y_prob, cost_matrix, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = y_sample\n",
    "Y = y_batch\n",
    "\n",
    "cost_matrix = squared_l2_matrix_np(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_X, psi_Y, _ = sinkhorn_dual_potentials_np(X, Y, epsilon, 100)\n",
    "ln_pi = log_coupling_np(psi_X, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_X_prob = - np.log(X.shape[0])\n",
    "log_Y_prob = - np.log(Y.shape[0])\n",
    "psi_Y = np.zeros(Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psi_X, psi_Y = sinkhorn_step_np(None, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon)\n",
    "for i in range(100):\n",
    "    psi_X_upd =  -epsilon*logsumexp(l+(np.expand_dims(psi_Y, axis=0) - cost_matrix) / epsilon, axis = 1)\n",
    "    psi_Y_upd =  -epsilon*logsumexp(l+(np.expand_dims(psi_X_upd, axis=1) - cost_matrix)/epsilon,axis = 0)\n",
    "    psi_X = psi_X_upd\n",
    "    psi_Y = psi_Y_upd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = -np.log(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_Y -= epsilon*np.log(BATCH_SIZE)/2\n",
    "psi_X -= epsilon*np.log(BATCH_SIZE)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.exp((- cost_matrix + np.expand_dims(psi_Y, axis=0) + np.expand_dims(psi_X, axis=1))/epsilon), axis = 1)/(BATCH_SIZE**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    psi_X, psi_Y, _ = sinkhorn_step_np(psi_X, psi_Y, log_X_prob, log_Y_prob, \\\n",
    "                                       cost_matrix, epsilon, return_diff = True)\n",
    "log_pi = log_coupling_np(psi_X, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.exp(log_pi), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_min(log_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = tf.exp(log_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = tf.exp(log_pi)\n",
    "loss = tf.reduce_sum(pi*(cost_matrix+epsilon*log_pi)) - epsilon*(log_X_prob + log_Y_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
