{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 64 # Model dimensionality (number of neurons in the hidden layer(s))\n",
    "CRITIC_ITERS = 10 # How many critic iterations (Sinkhorn iterations) per generator iteration#was 50\n",
    "BATCH_SIZE = 1024#256 # Batch size\n",
    "ITERS = 2500#100000 # how many generator iterations to train for\n",
    "DATA_DIM = 32\n",
    "LATENT_DIM = 4\n",
    "INITIALIZATION = 'he'#'glorot'\n",
    "COVARIANCE_SCALE = np.sqrt(DATA_DIM)\n",
    "INITIALIZE_LAST = True\n",
    "SAMPLE_SIZE = 100000\n",
    "LAMBDA = 0.3#2/(COVARIANCE_SCALE)\n",
    "MODE = 'divergence' #'loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INITIALIZATION == 'he':\n",
    "    weight_initializer = keras.initializers.he_uniform()\n",
    "if INITIALIZATION == 'glorot':\n",
    "    weight_initializer = keras.initializers.glorot_uniform()\n",
    "bias_initializer = keras.initializers.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator:\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 10,720\n",
      "Trainable params: 10,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#class WGAN:\n",
    " #   def __init__()self\n",
    "latent_sample = keras.Input(shape=(LATENT_DIM,))\n",
    "hidden_layer = latent_sample\n",
    "for i in range(3):\n",
    "    hidden_layer = layers.Dense(DIM, activation=\"relu\", kernel_initializer=weight_initializer,\n",
    "        bias_initializer=bias_initializer)(hidden_layer)\n",
    "output = layers.Dense(DATA_DIM, kernel_initializer=weight_initializer,\n",
    "                      bias_initializer=bias_initializer)(hidden_layer)\n",
    "generator = keras.Model(inputs=latent_sample, outputs = output, name=\"generator\")\n",
    "print('Generator:')\n",
    "print(generator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_l2_matrix_np(X, Y):\n",
    "    squared_l2_X = np.sum(np.square(X), axis = 1, keepdims=True)\n",
    "    squared_l2_Y = np.sum(np.square(Y), axis = 1, keepdims=True)\n",
    "    XY = X.dot(Y.T)\n",
    "    return squared_l2_X + tf.transpose(squared_l2_Y) - 2 * XY\n",
    "\n",
    "def log_coupling_np(psi_X, psi_Y, cost_matrix, epsilon):\n",
    "    C_tild = cost_matrix - np.expand_dims(psi_X, axis=1) - np.expand_dims(psi_Y, axis=0)\n",
    "    return -C_tild/epsilon\n",
    "\n",
    "def sinkhorn_step_np(psi_X, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon, return_diff = False):\n",
    "    psi_X_upd = epsilon * (log_X_prob - \\\n",
    "                       logsumexp((np.expand_dims(psi_Y, axis=0) - cost_matrix) / epsilon, axis = 1))\n",
    "    psi_Y_upd = epsilon * (log_Y_prob - \\\n",
    "                       logsumexp((np.expand_dims(psi_X_upd, axis=1) - cost_matrix)/epsilon,axis = 0))\n",
    "    if return_diff:\n",
    "        diff = np.linalg.norm(psi_X_upd - psi_X) + np.linalg.norm(psi_Y_upd - psi_Y)\n",
    "    psi_X = psi_X_upd\n",
    "    psi_Y = psi_Y_upd\n",
    "    if return_diff:\n",
    "        return psi_X, psi_Y, diff\n",
    "    return psi_X, psi_Y\n",
    "\n",
    "def sinkhorn_loss_np(X, Y, epsilon, num_steps):\n",
    "    cost_matrix = squared_l2_matrix_np(X,Y)\n",
    "    log_X_prob = - np.log(X.shape[0])\n",
    "    log_Y_prob = - np.log(Y.shape[0])\n",
    "    psi_Y = np.zeros(Y.shape[0])\n",
    "    for l in range(num_steps-1):\n",
    "        psi_X, psi_Y = sinkhorn_step_np(None, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon)\n",
    "    psi_X, psi_Y, diff = sinkhorn_step_np(psi_X, psi_Y, log_X_prob, log_Y_prob, \\\n",
    "                                       cost_matrix, epsilon, return_diff = True)\n",
    "    #return psi_X, psi_Y, diff\n",
    "    log_pi = log_coupling_np(psi_X, psi_Y, cost_matrix, epsilon)\n",
    "    pi = np.exp(log_pi)\n",
    "    loss = np.sum(pi*(cost_matrix+epsilon*log_pi)) - epsilon*(log_X_prob + log_Y_prob)\n",
    "    #X and Y are uniform, so entropy = -log(probability)\n",
    "    return loss, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_l2_matrix(X, Y):\n",
    "    squared_l2_X = tf.reduce_sum(tf.square(X), axis = 1, keepdims=True)\n",
    "    squared_l2_Y = tf.reduce_sum(tf.square(Y), axis = 1, keepdims=True)\n",
    "    XY = tf.matmul(X,tf.transpose(Y))\n",
    "    return squared_l2_X + tf.transpose(squared_l2_Y) - 2 * XY\n",
    "\n",
    "def log_coupling(psi_X, psi_Y, cost_matrix, epsilon):\n",
    "    C_tild = cost_matrix - tf.expand_dims(psi_X, axis=1) - tf.expand_dims(psi_Y, axis=0)\n",
    "    return -C_tild/epsilon\n",
    "\n",
    "def sinkhorn_step(psi_X, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon, return_diff = False):\n",
    "    psi_X_upd = epsilon * (log_X_prob - \\\n",
    "                       tf.reduce_logsumexp((tf.expand_dims(psi_Y, axis=0) - cost_matrix) / epsilon, axis = 1))\n",
    "    psi_Y_upd = epsilon * (log_Y_prob - \\\n",
    "                       tf.reduce_logsumexp((tf.expand_dims(psi_X_upd, axis=1) - cost_matrix)/epsilon,axis = 0))\n",
    "    if return_diff:\n",
    "        diff = tf.norm(psi_X_upd - psi_X) + tf.norm(psi_Y_upd - psi_Y)\n",
    "    psi_X = psi_X_upd\n",
    "    psi_Y = psi_Y_upd\n",
    "    if return_diff:\n",
    "        return psi_X, psi_Y, diff\n",
    "    return psi_X, psi_Y\n",
    "\n",
    "def sinkhorn_loss(X, Y, epsilon, num_steps, return_diff = False, return_diff_only = False):\n",
    "    cost_matrix = squared_l2_matrix(X,Y)\n",
    "    log_X_prob = - tf.math.log(tf.cast(tf.shape(X)[0], tf.float32))\n",
    "    log_Y_prob = - tf.math.log(tf.cast(tf.shape(Y)[0], tf.float32))\n",
    "    psi_Y = tf.zeros([tf.shape(Y)[0]])\n",
    "    for l in range(num_steps-1):\n",
    "        psi_X, psi_Y = sinkhorn_step(None, psi_Y, log_X_prob, log_Y_prob, cost_matrix, epsilon)\n",
    "    psi_X, psi_Y, diff = sinkhorn_step(psi_X, psi_Y, log_X_prob, log_Y_prob, \\\n",
    "                                       cost_matrix, epsilon, return_diff = True)\n",
    "    #return psi_X, psi_Y, diff\n",
    "    log_pi = log_coupling(psi_X, psi_Y, cost_matrix, epsilon)\n",
    "    pi = tf.exp(log_pi)\n",
    "    loss = tf.reduce_sum(pi*(cost_matrix+epsilon*log_pi)) - epsilon*(log_X_prob + log_Y_prob)\n",
    "    #X and Y are uniform, so entropy = -log(probability)\n",
    "    if return_diff_only:\n",
    "        return diff\n",
    "    if return_diff:\n",
    "        return loss, diff\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = LAMBDA\n",
    "if MODE == 'divergence':\n",
    "    model_loss = lambda x, y: sinkhorn_loss(x, y, epsilon, CRITIC_ITERS) - 0.5*\\\n",
    "        (sinkhorn_loss(x, x, epsilon, CRITIC_ITERS)-sinkhorn_loss(y, y, epsilon, CRITIC_ITERS))\n",
    "else:\n",
    "    model_loss = lambda x, y: sinkhorn_loss(x, y, epsilon, CRITIC_ITERS)\n",
    "\n",
    "if MODE == 'divergence':\n",
    "    args = [epsilon, CRITIC_ITERS, True, True]\n",
    "    sinkhorn_diff = lambda x, y: (sinkhorn_loss(x, y, *args)\\\n",
    "        +sinkhorn_loss(x, x, *args)+sinkhorn_loss(y, y, *args))/3\n",
    "else:\n",
    "    sinkhorn_diff = lambda x, y: sinkhorn_loss(x, y, epsilon, CRITIC_ITERS)\n",
    "\n",
    "metrics = lambda x, y: tf.norm(tfp.stats.covariance(x) - tfp.stats.covariance(y))\n",
    "generator.compile(optimizer=\"Adam\", loss=model_loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_train_gen():\n",
    "    np.random.seed(1)\n",
    "    full_dataset = np.random.randn(SAMPLE_SIZE,DATA_DIM) / np.sqrt(COVARIANCE_SCALE) \n",
    "    i = 0\n",
    "    offset = 0\n",
    "    while True:\n",
    "        dataset = full_dataset[i*BATCH_SIZE+offset:(i+1)*BATCH_SIZE+offset,:]\n",
    "        if (i+1)*BATCH_SIZE+offset > SAMPLE_SIZE: \n",
    "            offset = (i+1)*BATCH_SIZE+offset - SAMPLE_SIZE\n",
    "            np.random.shuffle(full_dataset)\n",
    "            dataset = np.concatenate([dataset,full_dataset[:offset,:]], axis = 0)\n",
    "            i = -1 \n",
    "        i+=1\n",
    "        yield dataset\n",
    "data_gen = inf_train_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch = next(data_gen)\n",
    "x_batch = np.random.normal(size=(y_batch.shape[0], LATENT_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample = generator.predict(x_batch, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801.1129233615842"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.cov(y_sample) - np.cov(y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=28.487322>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loss(tf.constant(y_sample, dtype='float32'), tf.constant(y_batch, dtype='float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_mat(X,Y,N,M): #for Sinkhorm divergence\n",
    "    XX = tf.reduce_sum(tf.multiply(X,X),axis=1)#norms of X\n",
    "    YY = tf.reduce_sum(tf.multiply(Y,Y),axis=1)#norms of Y\n",
    "    C1 = tf.transpose(tf.reshape(tf.tile(XX,[M]),[M,N]))\n",
    "    C2 = tf.reshape(tf.tile(YY,[N]),[N,M])\n",
    "    C3 = tf.transpose(tf.matmul(Y,tf.transpose(X)))\n",
    "    C = C1 + C2 - 2*C3; #squared norms of difference\n",
    "    return C\n",
    "\n",
    "def K_tild(u,v,C,N,M,epsilon):\n",
    "    C_tild = C - tf.transpose(tf.reshape(tf.tile(u[:,0],[M]),[M,N])) - tf.reshape(tf.tile(v[:,0],[N]),[N,M])\n",
    "    K_tild = tf.exp(-C_tild/epsilon)\n",
    "    return K_tild\n",
    "\n",
    "\n",
    "def log_K_tild(u,v,C,N,M,epsilon):\n",
    "    C_tild = C - tf.transpose(tf.reshape(tf.tile(u[:,0],[M]),[M,N])) - tf.reshape(tf.tile(v[:,0],[N]),[N,M])\n",
    "    return -C_tild/epsilon\n",
    "\n",
    "def sinkhorn_step_log(j,u,v,C, N,M,epsilon,diff,Lambda = 1):\n",
    "    mu = tf.cast(1/N, tf.float32)\n",
    "    nu = tf.cast(1/M, tf.float32)\n",
    "    Ku = tf.reshape( tf.reduce_sum(K_tild(u,v,C,N,M,epsilon),axis = 1) ,[N,1] )\n",
    "    u_new = Lambda * ( epsilon*(tf.math.log(mu) - tf.math.log(Ku +10**(-6))) + u )\n",
    "    diff = tf.norm(u_new - u)\n",
    "    u = u_new\n",
    "    \n",
    "    Kv = tf.reshape( tf.reduce_sum(K_tild(u,v,C,N,M,epsilon),axis = 0), [M,1] )\n",
    "    v_new = Lambda * ( epsilon*(tf.math.log(nu) - tf.math.log(Kv +10**(-6))) + v )\n",
    "    diff += tf.norm(v - v_new)\n",
    "    v = v_new\n",
    "    j += 1\n",
    "    return j,u,v,C,N,M,epsilon,diff\n",
    "\n",
    "def sinkhorn_loss1(X,Y):#LOSS, NOT DIVERGENCE\n",
    "    epsilon = tf.constant(LAMBDA, dtype=tf.float32) # smoothing sinkhorn\n",
    "    Lambda = tf.constant(1.) # unbalanced parameter\n",
    "    k = tf.constant(CRITIC_ITERS) # number of iterations for sinkhorn\n",
    "    N = tf.shape(X)[0] # sample size from mu_theta\n",
    "    M = tf.shape(Y)[0] # sample size from \\hat nu\n",
    "    \n",
    "    mu = tf.cast(1/N, tf.float32)\n",
    "    nu = tf.cast(1/M, tf.float32)\n",
    "    \n",
    "    D = tf.shape(Y)[1] # dimension of the obervation space\n",
    "    C = cost_mat(X,Y,N,M)\n",
    "    K = tf.exp(-C/epsilon)\n",
    "    #sinkhorn iterations\n",
    "    j0 = tf.constant(0)\n",
    "    u0 = tf.zeros([N,1])\n",
    "    v0 = tf.zeros([M,1])\n",
    "    diff = tf.cast(0., tf.float32)\n",
    "    cond_iter = lambda j, u, v, C, N, M, epsilon, diff: j < k\n",
    "    j,u,v,C,N,M,epsilon,diff = tf.while_loop(\n",
    "    cond_iter, sinkhorn_step_log, loop_vars=[j0, u0, v0,C, N,M,epsilon,diff])\n",
    "    gamma_log = K_tild(u,v,C,N,M,epsilon)\n",
    "    log_gamma_log = log_K_tild(u,v,C,N,M,epsilon)\n",
    "    final_cost = tf.reduce_sum(gamma_log*(C+epsilon*(log_gamma_log - tf.math.log(mu) - tf.math.log(nu))))\n",
    "    return final_cost, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(y_sample, dtype = 'float32')\n",
    "Y = tf.constant(y_batch, dtype = 'float32')\n",
    "loss_their = sinkhorn_loss1(X,Y)\n",
    "loss_my = sinkhorn_loss(X,Y, epsilon=LAMBDA, num_steps=CRITIC_ITERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "Training: loss 37.74919128417969, covariance difference 8.588347434997559\n",
      "Validation: loss 25.007077780213667, covariance difference 639.828849980119, sinkhorn epsilon 8.804343744228428\n",
      "Iteration  1\n",
      "Training: loss 31.41640281677246, covariance difference 7.111966133117676\n",
      "Validation: loss 23.21416514396919, covariance difference 530.1967824664677, sinkhorn epsilon 7.139346413990609\n",
      "Iteration  2\n",
      "Training: loss 27.95612144470215, covariance difference 6.575160980224609\n",
      "Validation: loss 21.57743422653142, covariance difference 444.7808165818492, sinkhorn epsilon 6.338020658890348\n",
      "Iteration  3\n",
      "Training: loss 25.23994255065918, covariance difference 6.028720378875732\n",
      "Validation: loss 19.01532778122752, covariance difference 334.78127336813276, sinkhorn epsilon 4.513818354099044\n",
      "Iteration  4\n",
      "Training: loss 21.1697940826416, covariance difference 5.084527015686035\n",
      "Validation: loss 16.630164845733763, covariance difference 265.7067050705343, sinkhorn epsilon 4.2487144999017525\n",
      "Iteration  5\n",
      "Training: loss 18.356321334838867, covariance difference 4.470338821411133\n",
      "Validation: loss 15.548239250283313, covariance difference 222.4629611751355, sinkhorn epsilon 3.3254484859447317\n",
      "Iteration  6\n",
      "Training: loss 16.605377197265625, covariance difference 3.9311482906341553\n",
      "Validation: loss 13.459342997687333, covariance difference 169.49246500361397, sinkhorn epsilon 3.55689774093715\n",
      "Iteration  7\n",
      "Training: loss 14.218994140625, covariance difference 3.3624823093414307\n",
      "Validation: loss 12.966633267378754, covariance difference 150.0643123158923, sinkhorn epsilon 2.2965836995827957\n",
      "Iteration  8\n",
      "Training: loss 13.437475204467773, covariance difference 3.1373953819274902\n",
      "Validation: loss 12.101401825249472, covariance difference 131.0042666214837, sinkhorn epsilon 1.7291814271748738\n",
      "Iteration  9\n",
      "Training: loss 12.439629554748535, covariance difference 2.915844440460205\n",
      "Validation: loss 11.106391982016227, covariance difference 110.6431840789276, sinkhorn epsilon 1.4090940847111881\n",
      "Iteration  10\n",
      "Training: loss 11.293318748474121, covariance difference 2.456930160522461\n",
      "Validation: loss 10.500086142276045, covariance difference 95.758811280476, sinkhorn epsilon 0.7901373957681783\n",
      "Iteration  11\n",
      "Training: loss 10.530435562133789, covariance difference 2.175973653793335\n",
      "Validation: loss 9.934747709929994, covariance difference 87.98180627913122, sinkhorn epsilon 1.040469890975523\n",
      "Iteration  12\n",
      "Training: loss 9.970067024230957, covariance difference 1.9540144205093384\n",
      "Validation: loss 9.234974514139001, covariance difference 79.18608139212351, sinkhorn epsilon 1.0450369109415636\n",
      "Iteration  13\n",
      "Training: loss 9.269718170166016, covariance difference 1.7700533866882324\n",
      "Validation: loss 9.015577294322025, covariance difference 74.51606010757939, sinkhorn epsilon 0.7264059511285164\n",
      "Iteration  14\n",
      "Training: loss 9.01280689239502, covariance difference 1.6528191566467285\n",
      "Validation: loss 8.703330106571233, covariance difference 68.50410230900633, sinkhorn epsilon 0.5503291532698973\n",
      "Iteration  15\n",
      "Training: loss 8.637479782104492, covariance difference 1.4844118356704712\n",
      "Validation: loss 8.041395324138396, covariance difference 60.46126196875298, sinkhorn epsilon 0.41379248453772377\n",
      "Iteration  16\n",
      "Training: loss 7.976170539855957, covariance difference 1.2565958499908447\n",
      "Validation: loss 7.994246579219837, covariance difference 59.8101916346217, sinkhorn epsilon 0.3379204924372008\n",
      "Iteration  17\n",
      "Training: loss 7.911501884460449, covariance difference 1.2620567083358765\n",
      "Validation: loss 7.637094800200802, covariance difference 54.96797851062812, sinkhorn epsilon 0.25483868203932286\n",
      "Iteration  18\n",
      "Training: loss 7.547244548797607, covariance difference 1.075439691543579\n",
      "Validation: loss 7.495878568261391, covariance difference 52.629700383192265, sinkhorn epsilon 0.17006106363537402\n",
      "Iteration  19\n",
      "Training: loss 7.390087127685547, covariance difference 1.0241882801055908\n",
      "Validation: loss 7.276910136247825, covariance difference 49.98480354167912, sinkhorn epsilon 0.17727585534268792\n",
      "Iteration  20\n",
      "Training: loss 7.164081573486328, covariance difference 0.9635522365570068\n",
      "Validation: loss 7.098275724520256, covariance difference 47.72194265522456, sinkhorn epsilon 0.12527488136177822\n",
      "Iteration  21\n",
      "Training: loss 6.978665828704834, covariance difference 0.9186804294586182\n",
      "Validation: loss 6.930084725957385, covariance difference 45.76525002186793, sinkhorn epsilon 0.1643060105462859\n",
      "Iteration  22\n",
      "Training: loss 6.799396514892578, covariance difference 0.8660566210746765\n",
      "Validation: loss 6.935754731511826, covariance difference 45.525049175486856, sinkhorn epsilon 0.2460909869929293\n",
      "Iteration  23\n",
      "Training: loss 6.798684120178223, covariance difference 0.8724092841148376\n",
      "Validation: loss 6.692578784434813, covariance difference 43.34994639704229, sinkhorn epsilon 0.1309355985289006\n",
      "Iteration  24\n",
      "Training: loss 6.554314613342285, covariance difference 0.816956102848053\n",
      "Validation: loss 6.644175190732474, covariance difference 42.34707580974051, sinkhorn epsilon 0.12573422020538064\n",
      "Iteration  25\n",
      "Training: loss 6.492786884307861, covariance difference 0.7929613590240479\n",
      "Validation: loss 6.461231826328896, covariance difference 40.02231233730139, sinkhorn epsilon 0.12822188590744799\n",
      "Iteration  26\n",
      "Training: loss 6.293320655822754, covariance difference 0.7798994779586792\n",
      "Validation: loss 6.452497912172068, covariance difference 40.17786535418874, sinkhorn epsilon 0.13295338314842386\n",
      "Iteration  27\n",
      "Training: loss 6.270045280456543, covariance difference 0.7806530594825745\n",
      "Validation: loss 6.323981806757233, covariance difference 38.96280774153033, sinkhorn epsilon 0.16301959841998348\n",
      "Iteration  28\n",
      "Training: loss 6.127923965454102, covariance difference 0.7759175300598145\n",
      "Validation: loss 6.244665551570339, covariance difference 38.52976256878459, sinkhorn epsilon 0.10296682159210313\n",
      "Iteration  29\n",
      "Training: loss 6.053001880645752, covariance difference 0.7610059380531311\n",
      "Validation: loss 6.114655155975208, covariance difference 37.104739275218925, sinkhorn epsilon 0.09867833855348565\n",
      "Iteration  30\n",
      "Training: loss 5.902560234069824, covariance difference 0.7665542364120483\n",
      "Validation: loss 6.073311419278729, covariance difference 36.74451417835529, sinkhorn epsilon 0.11001706602020006\n",
      "Iteration  31\n",
      "Training: loss 5.8461503982543945, covariance difference 0.7578021883964539\n",
      "Validation: loss 6.159666244464461, covariance difference 37.06445550288731, sinkhorn epsilon 0.0590518709180345\n",
      "Iteration  32\n",
      "Training: loss 5.917722702026367, covariance difference 0.7740316987037659\n",
      "Validation: loss 6.062814571085127, covariance difference 36.34933919552175, sinkhorn epsilon 0.19921015699269629\n",
      "Iteration  33\n",
      "Training: loss 5.797853946685791, covariance difference 0.7918026447296143\n",
      "Validation: loss 6.053054544100838, covariance difference 36.29367462546049, sinkhorn epsilon 0.038752402240989275\n",
      "Iteration  34\n",
      "Training: loss 5.796018123626709, covariance difference 0.7861593961715698\n",
      "Validation: loss 5.956436147391059, covariance difference 35.441814753306055, sinkhorn epsilon 0.12356642750807428\n",
      "Iteration  35\n",
      "Training: loss 5.662077903747559, covariance difference 0.7881369590759277\n",
      "Validation: loss 5.971075824020143, covariance difference 35.5088492973782, sinkhorn epsilon 0.056515956302139375\n",
      "Iteration  36\n",
      "Training: loss 5.686142444610596, covariance difference 0.7910196781158447\n",
      "Validation: loss 5.926250702007394, covariance difference 35.13970923214374, sinkhorn epsilon 0.1003639518813582\n",
      "Iteration  37\n",
      "Training: loss 5.611450672149658, covariance difference 0.7988052368164062\n",
      "Validation: loss 5.95435504173247, covariance difference 35.268534806339915, sinkhorn epsilon 0.07003259662837005\n",
      "Iteration  38\n",
      "Training: loss 5.649570465087891, covariance difference 0.8092579245567322\n",
      "Validation: loss 5.852041492441695, covariance difference 34.53665784891674, sinkhorn epsilon 0.06992674141606922\n",
      "Iteration  39\n",
      "Training: loss 5.514925479888916, covariance difference 0.8031745553016663\n",
      "Validation: loss 5.901759380978202, covariance difference 34.84666034733349, sinkhorn epsilon 0.051116539853374526\n",
      "Iteration  40\n",
      "Training: loss 5.557751178741455, covariance difference 0.8174118995666504\n",
      "Validation: loss 5.90680958307958, covariance difference 35.04091844958701, sinkhorn epsilon 0.030750572441091147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  41\n",
      "Training: loss 5.571918487548828, covariance difference 0.8134355545043945\n",
      "Validation: loss 5.86058089765997, covariance difference 34.774679257994435, sinkhorn epsilon 0.03824589523424893\n",
      "Iteration  42\n",
      "Training: loss 5.514727592468262, covariance difference 0.823392391204834\n",
      "Validation: loss 5.817507722233056, covariance difference 34.08927667514661, sinkhorn epsilon 0.08628549305521464\n",
      "Iteration  43\n",
      "Training: loss 5.432528495788574, covariance difference 0.8387850522994995\n",
      "Validation: loss 5.867210309481747, covariance difference 34.64568196914918, sinkhorn epsilon 0.09217519188227913\n",
      "Iteration  44\n",
      "Training: loss 5.47402811050415, covariance difference 0.8481283783912659\n",
      "Validation: loss 5.87071034917437, covariance difference 34.61756097686972, sinkhorn epsilon 0.027788824048907083\n",
      "Iteration  45\n",
      "Training: loss 5.476345539093018, covariance difference 0.8542080521583557\n",
      "Validation: loss 5.81745618965108, covariance difference 34.25037473430803, sinkhorn epsilon 0.16808638778183124\n",
      "Iteration  46\n",
      "Training: loss 5.390494346618652, covariance difference 0.8520230650901794\n",
      "Validation: loss 5.809040154699121, covariance difference 34.198533662377635, sinkhorn epsilon 0.024700989057739202\n",
      "Iteration  47\n",
      "Training: loss 5.3735127449035645, covariance difference 0.8636178970336914\n",
      "Validation: loss 5.722031186753393, covariance difference 33.604801044030694, sinkhorn epsilon 0.03449016637455169\n",
      "Iteration  48\n",
      "Training: loss 5.297947883605957, covariance difference 0.8414276242256165\n",
      "Validation: loss 5.74341245049266, covariance difference 33.705558519454506, sinkhorn epsilon 0.06735346159983299\n",
      "Iteration  49\n",
      "Training: loss 5.275128364562988, covariance difference 0.8721029758453369\n",
      "Validation: loss 5.75525878963097, covariance difference 33.7989464531202, sinkhorn epsilon 0.09144159506676294\n",
      "Iteration  50\n",
      "Training: loss 5.27140474319458, covariance difference 0.8773107528686523\n",
      "Validation: loss 5.6807861202558065, covariance difference 33.40368699510367, sinkhorn epsilon 0.07221625815365681\n",
      "Iteration  51\n",
      "Training: loss 5.195265293121338, covariance difference 0.8699033260345459\n",
      "Validation: loss 5.711754656873593, covariance difference 33.682142083364624, sinkhorn epsilon 0.08311990372699816\n",
      "Iteration  52\n",
      "Training: loss 5.210747718811035, covariance difference 0.8793330192565918\n",
      "Validation: loss 5.6866491485683275, covariance difference 33.37892513812212, sinkhorn epsilon 0.04464835603352811\n",
      "Iteration  53\n",
      "Training: loss 5.16989803314209, covariance difference 0.8788390159606934\n",
      "Validation: loss 5.674093442708591, covariance difference 33.29103754928914, sinkhorn epsilon 0.0639708283329973\n",
      "Iteration  54\n",
      "Training: loss 5.164532661437988, covariance difference 0.8823968768119812\n",
      "Validation: loss 5.69249196196469, covariance difference 33.37911419887435, sinkhorn epsilon 0.02236641435614102\n",
      "Iteration  55\n",
      "Training: loss 5.151927947998047, covariance difference 0.8917875289916992\n",
      "Validation: loss 5.67691422081866, covariance difference 33.295678049140285, sinkhorn epsilon 0.048129789556469856\n",
      "Iteration  56\n",
      "Training: loss 5.1517839431762695, covariance difference 0.8885232210159302\n",
      "Validation: loss 5.712791274779369, covariance difference 33.55577100337521, sinkhorn epsilon 0.03895099109341686\n",
      "Iteration  57\n",
      "Training: loss 5.159071922302246, covariance difference 0.9038622379302979\n",
      "Validation: loss 5.640996378314887, covariance difference 33.10326582284995, sinkhorn epsilon 0.018603309765534926\n",
      "Iteration  58\n",
      "Training: loss 5.073141574859619, covariance difference 0.8918255567550659\n",
      "Validation: loss 5.725982509309696, covariance difference 33.629581507369615, sinkhorn epsilon 0.024194147814627795\n",
      "Iteration  59\n",
      "Training: loss 5.1519880294799805, covariance difference 0.9162226915359497\n",
      "Validation: loss 5.739519788663327, covariance difference 33.79368557142043, sinkhorn epsilon 0.08407006288271696\n",
      "Iteration  60\n",
      "Training: loss 5.1485276222229, covariance difference 0.9250495433807373\n",
      "Validation: loss 5.679705529853192, covariance difference 33.3559328819611, sinkhorn epsilon 0.02311545774556033\n",
      "Iteration  61\n",
      "Training: loss 5.087985992431641, covariance difference 0.9077659845352173\n",
      "Validation: loss 5.663916502750023, covariance difference 33.17749851246381, sinkhorn epsilon 0.18406580289322538\n",
      "Iteration  62\n",
      "Training: loss 5.0584917068481445, covariance difference 0.9149682521820068\n",
      "Validation: loss 5.642169876455078, covariance difference 33.129723097668375, sinkhorn epsilon 0.021830480936647915\n",
      "Iteration  63\n",
      "Training: loss 5.0059404373168945, covariance difference 0.9213089346885681\n",
      "Validation: loss 5.745696281238097, covariance difference 33.602993825031525, sinkhorn epsilon 0.08168745611456579\n",
      "Iteration  64\n",
      "Training: loss 5.110060691833496, covariance difference 0.9327058792114258\n",
      "Validation: loss 5.669335968097181, covariance difference 33.2972988962307, sinkhorn epsilon 0.008958618988028823\n",
      "Iteration  65\n",
      "Training: loss 5.013622283935547, covariance difference 0.9319912791252136\n",
      "Validation: loss 5.66535559601178, covariance difference 33.214342990825564, sinkhorn epsilon 0.00997050693140354\n",
      "Iteration  66\n",
      "Training: loss 4.983606815338135, covariance difference 0.9340934753417969\n",
      "Validation: loss 5.6110411193006495, covariance difference 32.8929903772182, sinkhorn epsilon 0.02276421068219791\n",
      "Iteration  67\n",
      "Training: loss 4.925232887268066, covariance difference 0.9256312251091003\n",
      "Validation: loss 5.65658241406199, covariance difference 33.091737115535246, sinkhorn epsilon 0.005732212046464668\n",
      "Iteration  68\n",
      "Training: loss 4.972101211547852, covariance difference 0.9337157011032104\n",
      "Validation: loss 5.596227696678977, covariance difference 32.82300701367674, sinkhorn epsilon 0.04138075348167707\n",
      "Iteration  69\n",
      "Training: loss 4.882175445556641, covariance difference 0.935248076915741\n",
      "Validation: loss 5.588339082826953, covariance difference 32.68760022819783, sinkhorn epsilon 0.017312887541476488\n",
      "Iteration  70\n",
      "Training: loss 4.880462646484375, covariance difference 0.9290838837623596\n",
      "Validation: loss 5.661013580561645, covariance difference 33.160533844763364, sinkhorn epsilon 0.01942307864548335\n",
      "Iteration  71\n",
      "Training: loss 4.932708263397217, covariance difference 0.9500107169151306\n",
      "Validation: loss 5.659479035031434, covariance difference 33.068396780555474, sinkhorn epsilon 0.017762348480541028\n",
      "Iteration  72\n",
      "Training: loss 4.93756628036499, covariance difference 0.9485347867012024\n",
      "Validation: loss 5.605198923162277, covariance difference 32.76349900387096, sinkhorn epsilon 0.0076529572511391305\n",
      "Iteration  73\n",
      "Training: loss 4.863603591918945, covariance difference 0.9419552087783813\n",
      "Validation: loss 5.638971416651785, covariance difference 32.98974735380572, sinkhorn epsilon 0.010887226187523018\n",
      "Iteration  74\n",
      "Training: loss 4.891332626342773, covariance difference 0.9503976702690125\n",
      "Validation: loss 5.634539401318934, covariance difference 33.09916013484001, sinkhorn epsilon 0.009672643728739057\n",
      "Iteration  75\n",
      "Training: loss 4.864203453063965, covariance difference 0.957529604434967\n",
      "Validation: loss 5.740732039789162, covariance difference 33.696324367030265, sinkhorn epsilon 0.004429066868204245\n",
      "Iteration  76\n",
      "Training: loss 4.957402229309082, covariance difference 0.9796530604362488\n",
      "Validation: loss 5.684811482607115, covariance difference 33.33428171667493, sinkhorn epsilon 0.0074784256914937446\n",
      "Iteration  77\n",
      "Training: loss 4.883861541748047, covariance difference 0.9741788506507874\n",
      "Validation: loss 5.672435016402275, covariance difference 33.240352224220274, sinkhorn epsilon 0.005315841236753351\n",
      "Iteration  78\n",
      "Training: loss 4.8789873123168945, covariance difference 0.9703766703605652\n",
      "Validation: loss 5.693554820127092, covariance difference 33.33984015960432, sinkhorn epsilon 0.04378157884599407\n",
      "Iteration  79\n",
      "Training: loss 4.89393949508667, covariance difference 0.9739993810653687\n",
      "Validation: loss 5.622012974819775, covariance difference 32.937674098493105, sinkhorn epsilon 0.026030324702901796\n",
      "Iteration  80\n",
      "Training: loss 4.816061019897461, covariance difference 0.961132287979126\n",
      "Validation: loss 5.6972012382328145, covariance difference 33.30722361986192, sinkhorn epsilon 0.0027311620260447034\n",
      "Iteration  81\n",
      "Training: loss 4.861307621002197, covariance difference 0.982767641544342\n",
      "Validation: loss 5.655095475301357, covariance difference 33.15595415723684, sinkhorn epsilon 0.008405567525344122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  82\n",
      "Training: loss 4.823050022125244, covariance difference 0.9741846919059753\n",
      "Validation: loss 5.626899723962247, covariance difference 32.925259731146234, sinkhorn epsilon 0.003973317168813634\n",
      "Iteration  83\n",
      "Training: loss 4.780470848083496, covariance difference 0.9736776351928711\n",
      "Validation: loss 5.625148445920805, covariance difference 33.01283093022833, sinkhorn epsilon 0.0008437895346624984\n",
      "Iteration  84\n",
      "Training: loss 4.7795209884643555, covariance difference 0.9729753136634827\n",
      "Validation: loss 5.6186944263194585, covariance difference 32.8944899030192, sinkhorn epsilon 0.009862588354578504\n",
      "Iteration  85\n",
      "Training: loss 4.759727954864502, covariance difference 0.9734035134315491\n",
      "Validation: loss 5.680182304370346, covariance difference 33.227705940663675, sinkhorn epsilon 0.0005415776847359871\n",
      "Iteration  86\n",
      "Training: loss 4.823807239532471, covariance difference 0.982150137424469\n",
      "Validation: loss 5.690797902394639, covariance difference 33.33929366835476, sinkhorn epsilon 0.003061356539206135\n",
      "Iteration  87\n",
      "Training: loss 4.823907852172852, covariance difference 0.989916205406189\n",
      "Validation: loss 5.640499491840485, covariance difference 33.03579212616447, sinkhorn epsilon 0.003669636169954641\n",
      "Iteration  88\n",
      "Training: loss 4.7613372802734375, covariance difference 0.9820009469985962\n",
      "Validation: loss 5.6050864619273755, covariance difference 32.709716217742134, sinkhorn epsilon 0.014986511605016552\n",
      "Iteration  89\n",
      "Training: loss 4.726754188537598, covariance difference 0.9734581112861633\n",
      "Validation: loss 5.719108724194918, covariance difference 33.475512664804285, sinkhorn epsilon 0.0011688886145994064\n",
      "Iteration  90\n",
      "Training: loss 4.827962875366211, covariance difference 0.9988557696342468\n",
      "Validation: loss 5.66423359512045, covariance difference 33.18512083411014, sinkhorn epsilon 0.0012202747313852396\n",
      "Iteration  91\n",
      "Training: loss 4.768553733825684, covariance difference 0.9910141229629517\n",
      "Validation: loss 5.5496239210663125, covariance difference 32.40478815619733, sinkhorn epsilon 0.010641137507830044\n",
      "Iteration  92\n",
      "Training: loss 4.655844688415527, covariance difference 0.9680320024490356\n",
      "Validation: loss 5.777456100285003, covariance difference 33.82597983176987, sinkhorn epsilon 0.051308247739988794\n",
      "Iteration  93\n",
      "Training: loss 4.874721527099609, covariance difference 1.0093731880187988\n",
      "Validation: loss 5.604420014177861, covariance difference 32.723783847697376, sinkhorn epsilon 0.002990844110827161\n",
      "Iteration  94\n",
      "Training: loss 4.684147834777832, covariance difference 0.9821391701698303\n",
      "Validation: loss 5.608984316786397, covariance difference 32.837870579354714, sinkhorn epsilon 0.0008842912135716349\n",
      "Iteration  95\n",
      "Training: loss 4.695034980773926, covariance difference 0.9842501282691956\n",
      "Validation: loss 5.69793445595052, covariance difference 33.316611930131614, sinkhorn epsilon 0.001282034663536884\n",
      "Iteration  96\n",
      "Training: loss 4.788421630859375, covariance difference 0.9973758459091187\n",
      "Validation: loss 5.663623513265048, covariance difference 33.10422726075075, sinkhorn epsilon 0.007469870591058856\n",
      "Iteration  97\n",
      "Training: loss 4.7411274909973145, covariance difference 0.9935665726661682\n",
      "Validation: loss 5.7089064439730945, covariance difference 33.433458655704506, sinkhorn epsilon 0.00014141556070430185\n",
      "Iteration  98\n",
      "Training: loss 4.781101226806641, covariance difference 1.0061235427856445\n",
      "Validation: loss 5.6452274490034835, covariance difference 32.91311700810245, sinkhorn epsilon 0.0010316093816905248\n",
      "Iteration  99\n",
      "Training: loss 4.703287124633789, covariance difference 0.995002806186676\n",
      "Validation: loss 5.647507088789345, covariance difference 33.0198942461759, sinkhorn epsilon 0.0008973684156596533\n",
      "Iteration  100\n",
      "Training: loss 4.705151557922363, covariance difference 0.9951989650726318\n",
      "Validation: loss 5.636968021605455, covariance difference 32.92212135935708, sinkhorn epsilon 0.0049169008986119905\n",
      "Iteration  101\n",
      "Training: loss 4.684781551361084, covariance difference 0.9960215091705322\n",
      "Validation: loss 5.625432356496365, covariance difference 32.81494910615047, sinkhorn epsilon 0.000432966770812293\n",
      "Iteration  102\n",
      "Training: loss 4.671777248382568, covariance difference 0.9929456114768982\n",
      "Validation: loss 5.642798053587141, covariance difference 33.00385931964094, sinkhorn epsilon 0.0008102654230763796\n",
      "Iteration  103\n",
      "Training: loss 4.6904826164245605, covariance difference 0.9977582097053528\n",
      "Validation: loss 5.7318551857695565, covariance difference 33.533185524345626, sinkhorn epsilon 0.008878078021182624\n",
      "Iteration  104\n",
      "Training: loss 4.76551628112793, covariance difference 1.0162407159805298\n",
      "Validation: loss 5.657151627027409, covariance difference 33.07446830169966, sinkhorn epsilon 0.0001779862886194156\n",
      "Iteration  105\n",
      "Training: loss 4.700013637542725, covariance difference 1.0025066137313843\n",
      "Validation: loss 5.656166209465036, covariance difference 33.11217637335567, sinkhorn epsilon 1.0712620815135351e-05\n",
      "Iteration  106\n",
      "Training: loss 4.692978858947754, covariance difference 1.004761815071106\n",
      "Validation: loss 5.621858345283575, covariance difference 32.8757314099197, sinkhorn epsilon 0.004364636750355635\n",
      "Iteration  107\n",
      "Training: loss 4.660551071166992, covariance difference 0.994869589805603\n",
      "Validation: loss 5.578582744562396, covariance difference 32.598906855074745, sinkhorn epsilon 5.157608243058659e-05\n",
      "Iteration  108\n",
      "Training: loss 4.60974645614624, covariance difference 0.9879617094993591\n",
      "Validation: loss 5.622129033294743, covariance difference 32.779024704729, sinkhorn epsilon 0.004075170341685098\n",
      "Iteration  109\n",
      "Training: loss 4.654105186462402, covariance difference 0.9942550659179688\n",
      "Validation: loss 5.613652315308342, covariance difference 32.854596355872026, sinkhorn epsilon 3.803776769842141e-06\n",
      "Iteration  110\n",
      "Training: loss 4.634699821472168, covariance difference 0.9970288276672363\n",
      "Validation: loss 5.628067063094534, covariance difference 32.79627883189384, sinkhorn epsilon 0.0014005884883039272\n",
      "Iteration  111\n",
      "Training: loss 4.645314693450928, covariance difference 0.9999187588691711\n",
      "Validation: loss 5.662062775484635, covariance difference 33.12310238996546, sinkhorn epsilon 1.0340996422647849e-05\n",
      "Iteration  112\n",
      "Training: loss 4.684931755065918, covariance difference 1.0056488513946533\n",
      "Validation: loss 5.684557989540046, covariance difference 33.0659680216902, sinkhorn epsilon 2.2651466170139268e-05\n",
      "Iteration  113\n",
      "Training: loss 4.699608325958252, covariance difference 1.0099050998687744\n",
      "Validation: loss 5.650824978314041, covariance difference 33.03316016302496, sinkhorn epsilon 6.871240931454695e-05\n",
      "Iteration  114\n",
      "Training: loss 4.6639018058776855, covariance difference 1.006030797958374\n",
      "Validation: loss 5.701427399082931, covariance difference 33.19143679329306, sinkhorn epsilon 0.0014259044133593424\n",
      "Iteration  115\n",
      "Training: loss 4.714602470397949, covariance difference 1.0145385265350342\n",
      "Validation: loss 5.679013155247022, covariance difference 33.101661270610315, sinkhorn epsilon 2.8943081729668095e-05\n",
      "Iteration  116\n",
      "Training: loss 4.685248851776123, covariance difference 1.0113190412521362\n",
      "Validation: loss 5.568183046091448, covariance difference 32.421158886758874, sinkhorn epsilon 3.877723117996831e-05\n",
      "Iteration  117\n",
      "Training: loss 4.578566551208496, covariance difference 0.9894270896911621\n",
      "Validation: loss 5.637145748195653, covariance difference 32.84918862995927, sinkhorn epsilon 0.00030601335465833704\n",
      "Iteration  118\n",
      "Training: loss 4.645345687866211, covariance difference 1.0016841888427734\n",
      "Validation: loss 5.642281941988738, covariance difference 32.96753749786016, sinkhorn epsilon 1.7682309118500347e-06\n",
      "Iteration  119\n",
      "Training: loss 4.640565872192383, covariance difference 1.0065239667892456\n",
      "Validation: loss 5.625888413168912, covariance difference 32.8414139956827, sinkhorn epsilon 0.0003771971344953657\n",
      "Iteration  120\n",
      "Training: loss 4.634718894958496, covariance difference 1.0023448467254639\n",
      "Validation: loss 5.69503557484447, covariance difference 33.28162673648599, sinkhorn epsilon 0.0001669992081577241\n",
      "Iteration  121\n",
      "Training: loss 4.695352554321289, covariance difference 1.0156196355819702\n",
      "Validation: loss 5.651014695778919, covariance difference 33.02234259934098, sinkhorn epsilon 1.2467694860059068e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  122\n",
      "Training: loss 4.646111488342285, covariance difference 1.008522391319275\n",
      "Validation: loss 5.672703170742475, covariance difference 33.063296502061135, sinkhorn epsilon 0.0003255529351237172\n",
      "Iteration  123\n",
      "Training: loss 4.670071125030518, covariance difference 1.0121859312057495\n",
      "Validation: loss 5.654226913226512, covariance difference 33.083823068933626, sinkhorn epsilon 3.130682922469983e-05\n",
      "Iteration  124\n",
      "Training: loss 4.653210639953613, covariance difference 1.0100913047790527\n",
      "Validation: loss 5.517174886088744, covariance difference 32.175440691190225, sinkhorn epsilon 0.0005361614588134216\n",
      "Iteration  125\n",
      "Training: loss 4.513741493225098, covariance difference 0.9825506806373596\n",
      "Validation: loss 5.618091446327301, covariance difference 32.748191887066255, sinkhorn epsilon 2.2966691325535604e-06\n",
      "Iteration  126\n",
      "Training: loss 4.610605716705322, covariance difference 1.0023987293243408\n",
      "Validation: loss 5.65691151320774, covariance difference 33.09238356241306, sinkhorn epsilon 6.65529772788768e-07\n",
      "Iteration  127\n",
      "Training: loss 4.643312454223633, covariance difference 1.0122524499893188\n",
      "Validation: loss 5.632654360874681, covariance difference 32.846632812759466, sinkhorn epsilon 9.714697604864051e-09\n",
      "Iteration  128\n",
      "Training: loss 4.617892265319824, covariance difference 1.0065244436264038\n",
      "Validation: loss 5.603445571336131, covariance difference 32.68624464873501, sinkhorn epsilon 3.1015432085695294e-05\n",
      "Iteration  129\n",
      "Training: loss 4.589872360229492, covariance difference 0.9997987747192383\n",
      "Validation: loss 5.651860520650485, covariance difference 32.94565183873963, sinkhorn epsilon 8.691760278707052e-07\n",
      "Iteration  130\n",
      "Training: loss 4.631818771362305, covariance difference 1.0092190504074097\n",
      "Validation: loss 5.701109574035101, covariance difference 33.237161777360605, sinkhorn epsilon 7.345316801398803e-05\n",
      "Iteration  131\n",
      "Training: loss 4.682657718658447, covariance difference 1.0185211896896362\n",
      "Validation: loss 5.650202294259502, covariance difference 33.01565342187654, sinkhorn epsilon 5.7232721273318025e-08\n",
      "Iteration  132\n",
      "Training: loss 4.630073547363281, covariance difference 1.0113214254379272\n",
      "Validation: loss 5.63976195398266, covariance difference 32.957545709505055, sinkhorn epsilon 1.8946525533910103e-06\n",
      "Iteration  133\n",
      "Training: loss 4.619139194488525, covariance difference 1.0090328454971313\n",
      "Validation: loss 5.828864806520119, covariance difference 34.00021419310165, sinkhorn epsilon 6.672463964512921e-09\n",
      "Iteration  134\n",
      "Training: loss 4.809067249298096, covariance difference 1.0428751707077026\n",
      "Validation: loss 5.657170171921264, covariance difference 33.06446309702337, sinkhorn epsilon 1.0426641777353777e-06\n",
      "Iteration  135\n",
      "Training: loss 4.638922691345215, covariance difference 1.011523723602295\n",
      "Validation: loss 5.703534550671394, covariance difference 33.32816756369959, sinkhorn epsilon 5.020452708260605e-09\n",
      "Iteration  136\n",
      "Training: loss 4.683897495269775, covariance difference 1.0177110433578491\n",
      "Validation: loss 5.688229236984104, covariance difference 33.18218703361825, sinkhorn epsilon 1.823719600521121e-05\n",
      "Iteration  137\n",
      "Training: loss 4.666518211364746, covariance difference 1.0168702602386475\n",
      "Validation: loss 5.659390510185777, covariance difference 33.00404630962196, sinkhorn epsilon 8.601786406284347e-08\n",
      "Iteration  138\n",
      "Training: loss 4.642152309417725, covariance difference 1.0114350318908691\n",
      "Validation: loss 5.590229494086307, covariance difference 32.67633489415473, sinkhorn epsilon 5.211477928530258e-09\n",
      "Iteration  139\n",
      "Training: loss 4.566512584686279, covariance difference 0.9998993873596191\n",
      "Validation: loss 5.646615552698963, covariance difference 32.922670693994235, sinkhorn epsilon 5.537014859445553e-10\n",
      "Iteration  140\n",
      "Training: loss 4.619596481323242, covariance difference 1.010892629623413\n",
      "Validation: loss 5.665014699739848, covariance difference 32.98609622322599, sinkhorn epsilon 1.3209634579284103e-10\n",
      "Iteration  141\n",
      "Training: loss 4.639396667480469, covariance difference 1.012694239616394\n",
      "Validation: loss 5.739737259783908, covariance difference 33.458312526680906, sinkhorn epsilon 3.5756543244738325e-05\n",
      "Iteration  142\n",
      "Training: loss 4.717047214508057, covariance difference 1.0256725549697876\n",
      "Validation: loss 5.602209501808804, covariance difference 32.70584067316914, sinkhorn epsilon 0.000237974244478931\n",
      "Iteration  143\n",
      "Training: loss 4.577967166900635, covariance difference 1.0029281377792358\n",
      "Validation: loss 5.587918026144981, covariance difference 32.51221959439324, sinkhorn epsilon 1.727876757498459e-11\n",
      "Iteration  144\n",
      "Training: loss 4.56234884262085, covariance difference 0.9992426037788391\n",
      "Validation: loss 5.5813468972912785, covariance difference 32.583787072003574, sinkhorn epsilon 1.1783610732925419e-07\n",
      "Iteration  145\n",
      "Training: loss 4.552384853363037, covariance difference 0.9984310269355774\n",
      "Validation: loss 5.654992481681344, covariance difference 32.988862864098735, sinkhorn epsilon 4.110053933556026e-10\n",
      "Iteration  146\n",
      "Training: loss 4.627821922302246, covariance difference 1.0117299556732178\n",
      "Validation: loss 5.718356816423528, covariance difference 33.36632083557752, sinkhorn epsilon 5.574627986272917e-13\n",
      "Iteration  147\n",
      "Training: loss 4.6874799728393555, covariance difference 1.023349404335022\n",
      "Validation: loss 5.600466656064689, covariance difference 32.74247455524036, sinkhorn epsilon 1.6189803968585432e-07\n",
      "Iteration  148\n",
      "Training: loss 4.571658134460449, covariance difference 1.0029716491699219\n",
      "Validation: loss 5.73564417033264, covariance difference 33.47481012813787, sinkhorn epsilon 1.3102573795822228e-10\n",
      "Iteration  149\n",
      "Training: loss 4.708906173706055, covariance difference 1.0268114805221558\n",
      "Validation: loss 5.684957223198509, covariance difference 33.23641282010728, sinkhorn epsilon 4.482545492990999e-10\n",
      "Iteration  150\n",
      "Training: loss 4.656790733337402, covariance difference 1.0184626579284668\n",
      "Validation: loss 5.735336315468022, covariance difference 33.44653192617827, sinkhorn epsilon 1.8704408809252977e-06\n",
      "Iteration  151\n",
      "Training: loss 4.708583831787109, covariance difference 1.0268365144729614\n",
      "Validation: loss 5.582336549595732, covariance difference 32.564915273239855, sinkhorn epsilon 5.384654872423072e-11\n",
      "Iteration  152\n",
      "Training: loss 4.551855087280273, covariance difference 0.9986076354980469\n",
      "Validation: loss 5.674725958581267, covariance difference 33.12729864547268, sinkhorn epsilon 8.430250052946371e-11\n",
      "Iteration  153\n",
      "Training: loss 4.645421981811523, covariance difference 1.0152721405029297\n",
      "Validation: loss 5.661110603483213, covariance difference 32.97193279791127, sinkhorn epsilon 4.2870207370775144e-14\n",
      "Iteration  154\n",
      "Training: loss 4.627603530883789, covariance difference 1.013648509979248\n",
      "Validation: loss 5.645199380578784, covariance difference 32.96472916030056, sinkhorn epsilon 3.401035514069414e-11\n",
      "Iteration  155\n",
      "Training: loss 4.614055633544922, covariance difference 1.010182499885559\n",
      "Validation: loss 5.6226722630376, covariance difference 32.79709774674518, sinkhorn epsilon 4.267151916939328e-14\n",
      "Iteration  156\n",
      "Training: loss 4.589032173156738, covariance difference 1.0085035562515259\n",
      "Validation: loss 5.62058535828522, covariance difference 32.82229553759521, sinkhorn epsilon 6.429211349827926e-08\n",
      "Iteration  157\n",
      "Training: loss 4.588685512542725, covariance difference 1.00711989402771\n",
      "Validation: loss 5.674218587977374, covariance difference 32.998436898612376, sinkhorn epsilon 1.5850700052523938e-13\n",
      "Iteration  158\n",
      "Training: loss 4.6414690017700195, covariance difference 1.0174251794815063\n",
      "Validation: loss 5.676651722909808, covariance difference 33.18725248866257, sinkhorn epsilon 4.79511730907528e-10\n",
      "Iteration  159\n",
      "Training: loss 4.645111083984375, covariance difference 1.0175776481628418\n",
      "Validation: loss 5.623145562023893, covariance difference 32.82555835958744, sinkhorn epsilon 1.3275872672535886e-13\n",
      "Iteration  160\n",
      "Training: loss 4.590950965881348, covariance difference 1.007655382156372\n",
      "Validation: loss 5.592076848872293, covariance difference 32.62451419351801, sinkhorn epsilon 7.13049668511847e-14\n",
      "Iteration  161\n",
      "Training: loss 4.559642314910889, covariance difference 1.0020686388015747\n",
      "Validation: loss 5.629132146344632, covariance difference 32.82942864162519, sinkhorn epsilon 2.0425720855462498e-13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  162\n",
      "Training: loss 4.594809532165527, covariance difference 1.0082141160964966\n",
      "Validation: loss 5.697498813427376, covariance difference 33.31276838891705, sinkhorn epsilon 5.219799492429218e-08\n",
      "Iteration  163\n",
      "Training: loss 4.6634931564331055, covariance difference 1.0223615169525146\n",
      "Validation: loss 5.6927263927181935, covariance difference 33.210091857654156, sinkhorn epsilon 2.375479258965467e-13\n",
      "Iteration  164\n",
      "Training: loss 4.659740447998047, covariance difference 1.0208648443222046\n",
      "Validation: loss 5.67690097728609, covariance difference 33.13003023923635, sinkhorn epsilon 2.491775177880678e-07\n",
      "Iteration  165\n",
      "Training: loss 4.643725395202637, covariance difference 1.016624927520752\n",
      "Validation: loss 5.693576392794467, covariance difference 33.260687163542826, sinkhorn epsilon 1.0344007412938269e-13\n",
      "Iteration  166\n",
      "Training: loss 4.659294128417969, covariance difference 1.0200093984603882\n",
      "Validation: loss 5.6576503626981545, covariance difference 33.035130455053974, sinkhorn epsilon 5.586857421543184e-11\n",
      "Iteration  167\n",
      "Training: loss 4.623905181884766, covariance difference 1.0139522552490234\n",
      "Validation: loss 5.691016949878694, covariance difference 33.18460362297158, sinkhorn epsilon 4.960333968766611e-14\n",
      "Iteration  168\n",
      "Training: loss 4.657488822937012, covariance difference 1.0191367864608765\n",
      "Validation: loss 5.631076907588973, covariance difference 32.82472468253651, sinkhorn epsilon 5.816280365500871e-14\n",
      "Iteration  169\n",
      "Training: loss 4.595005035400391, covariance difference 1.0097107887268066\n",
      "Validation: loss 5.698949401382266, covariance difference 33.22725792094966, sinkhorn epsilon 1.0984922181455701e-13\n",
      "Iteration  170\n",
      "Training: loss 4.663145065307617, covariance difference 1.0226984024047852\n",
      "Validation: loss 5.680210842546398, covariance difference 33.08557883481775, sinkhorn epsilon 4.230622372210462e-14\n",
      "Iteration  171\n",
      "Training: loss 4.644098281860352, covariance difference 1.01743745803833\n",
      "Validation: loss 5.583647358904543, covariance difference 32.58691238933876, sinkhorn epsilon 1.3876661242683578e-13\n",
      "Iteration  172\n",
      "Training: loss 4.548899173736572, covariance difference 1.0001667737960815\n",
      "Validation: loss 5.606022291601804, covariance difference 32.78631565777606, sinkhorn epsilon 5.3975429067390384e-14\n",
      "Iteration  173\n",
      "Training: loss 4.57110071182251, covariance difference 1.0067989826202393\n",
      "Validation: loss 5.699011515740523, covariance difference 33.26142618609927, sinkhorn epsilon 1.9147226459974935e-13\n",
      "Iteration  174\n",
      "Training: loss 4.664729118347168, covariance difference 1.0215997695922852\n",
      "Validation: loss 5.627513649676604, covariance difference 32.90349029158906, sinkhorn epsilon 5.73557779786004e-14\n",
      "Iteration  175\n",
      "Training: loss 4.592841625213623, covariance difference 1.008327841758728\n",
      "Validation: loss 5.6507502069876745, covariance difference 33.07741361129792, sinkhorn epsilon 7.501679009245233e-14\n",
      "Iteration  176\n",
      "Training: loss 4.614750862121582, covariance difference 1.0144548416137695\n",
      "Validation: loss 5.675142347162369, covariance difference 33.05677261989536, sinkhorn epsilon 5.6745376028702266e-14\n",
      "Iteration  177\n",
      "Training: loss 4.640166759490967, covariance difference 1.017144799232483\n",
      "Validation: loss 5.659893507164799, covariance difference 33.01943263559289, sinkhorn epsilon 2.9267929095210777e-14\n",
      "Iteration  178\n",
      "Training: loss 4.624129295349121, covariance difference 1.0148961544036865\n",
      "Validation: loss 5.604998776868389, covariance difference 32.72388127419404, sinkhorn epsilon 1.0105106191114236e-13\n",
      "Iteration  179\n",
      "Training: loss 4.569726467132568, covariance difference 1.0051511526107788\n",
      "Validation: loss 5.64671324837941, covariance difference 32.99524252932337, sinkhorn epsilon 3.78651162003324e-14\n",
      "Iteration  180\n",
      "Training: loss 4.6110663414001465, covariance difference 1.0134167671203613\n",
      "Validation: loss 5.622016871460038, covariance difference 32.813031923613586, sinkhorn epsilon 2.2223116420989758e-14\n",
      "Iteration  181\n",
      "Training: loss 4.584623336791992, covariance difference 1.0088430643081665\n",
      "Validation: loss 5.702713795341815, covariance difference 33.30444081593102, sinkhorn epsilon 9.239112899252348e-15\n",
      "Iteration  182\n",
      "Training: loss 4.666032791137695, covariance difference 1.0233997106552124\n",
      "Validation: loss 5.705862923503962, covariance difference 33.35861485674636, sinkhorn epsilon 2.773651277263066e-14\n",
      "Iteration  183\n",
      "Training: loss 4.669524192810059, covariance difference 1.0247057676315308\n",
      "Validation: loss 5.605001201586946, covariance difference 32.676940471960215, sinkhorn epsilon 2.6268621526524628e-14\n",
      "Iteration  184\n",
      "Training: loss 4.568320274353027, covariance difference 1.0040186643600464\n",
      "Validation: loss 5.616548767154212, covariance difference 32.72915426789376, sinkhorn epsilon 5.754984022919484e-14\n",
      "Iteration  185\n",
      "Training: loss 4.579788684844971, covariance difference 1.0052298307418823\n",
      "Validation: loss 5.656968842326783, covariance difference 33.02518444008658, sinkhorn epsilon 1.240016019391895e-13\n",
      "Iteration  186\n",
      "Training: loss 4.6204514503479, covariance difference 1.0139667987823486\n",
      "Validation: loss 5.708707810152915, covariance difference 33.32987566636766, sinkhorn epsilon 2.29101770069053e-15\n",
      "Iteration  187\n",
      "Training: loss 4.671436309814453, covariance difference 1.0230984687805176\n",
      "Validation: loss 5.748993966721039, covariance difference 33.54370243959554, sinkhorn epsilon 2.0775039951207427e-14\n",
      "Iteration  188\n",
      "Training: loss 4.712252616882324, covariance difference 1.0319961309432983\n",
      "Validation: loss 5.633747284943345, covariance difference 32.97182351046448, sinkhorn epsilon 3.566188971704204e-14\n",
      "Iteration  189\n",
      "Training: loss 4.597886085510254, covariance difference 1.0100358724594116\n",
      "Validation: loss 5.697624164702354, covariance difference 33.168106227700925, sinkhorn epsilon 5.949979769853737e-14\n",
      "Iteration  190\n",
      "Training: loss 4.660595417022705, covariance difference 1.0205910205841064\n",
      "Validation: loss 5.639325588075113, covariance difference 32.80547947000546, sinkhorn epsilon 6.437420154165524e-15\n",
      "Iteration  191\n",
      "Training: loss 4.60219144821167, covariance difference 1.0096421241760254\n",
      "Validation: loss 5.591457457083415, covariance difference 32.590977390375684, sinkhorn epsilon 3.337459875195648e-14\n",
      "Iteration  192\n",
      "Training: loss 4.554299354553223, covariance difference 1.003600835800171\n",
      "Validation: loss 5.6798168250548775, covariance difference 33.18565857073462, sinkhorn epsilon 4.530970647471967e-14\n",
      "Iteration  193\n",
      "Training: loss 4.643075942993164, covariance difference 1.0180917978286743\n",
      "Validation: loss 5.634939552182662, covariance difference 32.987912756164505, sinkhorn epsilon 1.4570277521605117e-15\n",
      "Iteration  194\n",
      "Training: loss 4.596975803375244, covariance difference 1.012749433517456\n",
      "Validation: loss 5.662832002264079, covariance difference 33.010592803445974, sinkhorn epsilon 1.3598762789788535e-15\n",
      "Iteration  195\n",
      "Training: loss 4.625716209411621, covariance difference 1.0170049667358398\n",
      "Validation: loss 5.718632951560725, covariance difference 33.32614494925395, sinkhorn epsilon 5.727825146100882e-13\n",
      "Iteration  196\n",
      "Training: loss 4.68134069442749, covariance difference 1.0243169069290161\n",
      "Validation: loss 5.677616953765317, covariance difference 33.110592778936265, sinkhorn epsilon 4.026601688501896e-14\n",
      "Iteration  197\n",
      "Training: loss 4.640117645263672, covariance difference 1.016394853591919\n",
      "Validation: loss 5.693313720053502, covariance difference 33.244850744029684, sinkhorn epsilon 2.7351966096781322e-14\n",
      "Iteration  198\n",
      "Training: loss 4.6555891036987305, covariance difference 1.0195645093917847\n",
      "Validation: loss 5.618375702827085, covariance difference 32.80241136633697, sinkhorn epsilon 1.6559374210500985e-14\n",
      "Iteration  199\n",
      "Training: loss 4.580695629119873, covariance difference 1.0078058242797852\n",
      "Validation: loss 5.679337507192017, covariance difference 33.09438011580731, sinkhorn epsilon 2.226799085375601e-15\n",
      "Iteration  200\n",
      "Training: loss 4.641953468322754, covariance difference 1.0170214176177979\n",
      "Validation: loss 5.560759509627699, covariance difference 32.46122609815417, sinkhorn epsilon 7.93399758194759e-15\n",
      "Iteration  201\n",
      "Training: loss 4.523536205291748, covariance difference 0.9970101118087769\n",
      "Validation: loss 5.6224910351960595, covariance difference 32.91829845315142, sinkhorn epsilon 2.0572463291507028e-14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  202\n",
      "Training: loss 4.58421516418457, covariance difference 1.0097519159317017\n",
      "Validation: loss 5.639323500043002, covariance difference 32.89797767048951, sinkhorn epsilon 4.228049100380474e-15\n",
      "Iteration  203\n",
      "Training: loss 4.601114749908447, covariance difference 1.0116932392120361\n",
      "Validation: loss 5.691661465764227, covariance difference 33.23179055870439, sinkhorn epsilon 1.8112397691977804e-14\n",
      "Iteration  204\n",
      "Training: loss 4.654012680053711, covariance difference 1.0218461751937866\n",
      "Validation: loss 5.719849097036582, covariance difference 33.38482886657158, sinkhorn epsilon 9.051038349787262e-15\n",
      "Iteration  205\n",
      "Training: loss 4.681933403015137, covariance difference 1.025163173675537\n",
      "Validation: loss 5.647175500035607, covariance difference 32.93832813452978, sinkhorn epsilon 2.0097844821418505e-14\n",
      "Iteration  206\n",
      "Training: loss 4.610136032104492, covariance difference 1.013540267944336\n",
      "Validation: loss 5.740390318755755, covariance difference 33.50022814831523, sinkhorn epsilon 3.375460954633763e-14\n",
      "Iteration  207\n",
      "Training: loss 4.702653884887695, covariance difference 1.029737114906311\n",
      "Validation: loss 5.70550223059979, covariance difference 33.20220706345852, sinkhorn epsilon 2.482908325061688e-14\n",
      "Iteration  208\n",
      "Training: loss 4.667052268981934, covariance difference 1.0227748155593872\n",
      "Validation: loss 5.692524934920412, covariance difference 33.20961490921933, sinkhorn epsilon 1.689012595512228e-14\n",
      "Iteration  209\n",
      "Training: loss 4.654542446136475, covariance difference 1.0209554433822632\n",
      "Validation: loss 5.603131123327378, covariance difference 32.7848147977821, sinkhorn epsilon 4.118296753448252e-15\n",
      "Iteration  210\n",
      "Training: loss 4.565289497375488, covariance difference 1.004729151725769\n",
      "Validation: loss 5.629592268304899, covariance difference 32.731145861517106, sinkhorn epsilon 4.03759595810021e-14\n",
      "Iteration  211\n",
      "Training: loss 4.591479301452637, covariance difference 1.0085986852645874\n",
      "Validation: loss 5.635209139836128, covariance difference 32.911772637291485, sinkhorn epsilon 1.6210655178040024e-14\n",
      "Iteration  212\n",
      "Training: loss 4.597051620483398, covariance difference 1.0105323791503906\n",
      "Validation: loss 5.606024434512747, covariance difference 32.75522975760176, sinkhorn epsilon 3.2879627481744092e-15\n",
      "Iteration  213\n",
      "Training: loss 4.567533493041992, covariance difference 1.005539894104004\n",
      "Validation: loss 5.640637644233133, covariance difference 32.82576716416665, sinkhorn epsilon 1.4353562920694418e-14\n",
      "Iteration  214\n",
      "Training: loss 4.602062702178955, covariance difference 1.010422945022583\n",
      "Validation: loss 5.618494543563432, covariance difference 32.82198806260416, sinkhorn epsilon 2.8106734978471664e-15\n",
      "Iteration  215\n",
      "Training: loss 4.580336570739746, covariance difference 1.0093642473220825\n",
      "Validation: loss 5.7190140830887835, covariance difference 33.35649933778555, sinkhorn epsilon 2.6346193247724865e-15\n",
      "Iteration  216\n",
      "Training: loss 4.681117534637451, covariance difference 1.0243780612945557\n",
      "Validation: loss 5.700723342611172, covariance difference 33.25760867105418, sinkhorn epsilon 3.720798332695417e-14\n",
      "Iteration  217\n",
      "Training: loss 4.662212371826172, covariance difference 1.021849513053894\n",
      "Validation: loss 5.612596046348205, covariance difference 32.71007903173978, sinkhorn epsilon 3.951748925786246e-14\n",
      "Iteration  218\n",
      "Training: loss 4.57428503036499, covariance difference 1.0061994791030884\n",
      "Validation: loss 5.640017823077077, covariance difference 32.936679597404705, sinkhorn epsilon 1.123656822190645e-14\n",
      "Iteration  219\n",
      "Training: loss 4.6013383865356445, covariance difference 1.0124485492706299\n",
      "Validation: loss 5.7235202925773985, covariance difference 33.33286446577349, sinkhorn epsilon 1.6115187648436874e-15\n",
      "Iteration  220\n",
      "Training: loss 4.685063362121582, covariance difference 1.0262179374694824\n",
      "Validation: loss 5.655245721495928, covariance difference 32.86829967932024, sinkhorn epsilon 1.5144776067401672e-15\n",
      "Iteration  221\n",
      "Training: loss 4.61698055267334, covariance difference 1.012585997581482\n",
      "Validation: loss 5.625872300164458, covariance difference 32.84133988075015, sinkhorn epsilon 2.6634038380572465e-14\n",
      "Iteration  222\n",
      "Training: loss 4.587221622467041, covariance difference 1.008783221244812\n",
      "Validation: loss 5.676507123501481, covariance difference 33.150801536728814, sinkhorn epsilon 7.995340077467299e-16\n",
      "Iteration  223\n",
      "Training: loss 4.637944221496582, covariance difference 1.0177899599075317\n",
      "Validation: loss 5.69539356999059, covariance difference 33.26254807043916, sinkhorn epsilon 1.1279730798862735e-14\n",
      "Iteration  224\n",
      "Training: loss 4.656709671020508, covariance difference 1.020971655845642\n",
      "Validation: loss 5.6876246518031355, covariance difference 33.211406560236036, sinkhorn epsilon 0.0\n",
      "Iteration  225\n",
      "Training: loss 4.649074077606201, covariance difference 1.0200871229171753\n",
      "Validation: loss 5.686368199508302, covariance difference 33.198549343126324, sinkhorn epsilon 2.6021055382198967e-14\n",
      "Iteration  226\n",
      "Training: loss 4.647274971008301, covariance difference 1.0221697092056274\n",
      "Validation: loss 5.6389221163845695, covariance difference 32.94480551246426, sinkhorn epsilon 1.1134066843685548e-14\n",
      "Iteration  227\n",
      "Training: loss 4.600446701049805, covariance difference 1.012437105178833\n",
      "Validation: loss 5.570068303917006, covariance difference 32.515842686772466, sinkhorn epsilon 1.6710235853254773e-14\n",
      "Iteration  228\n",
      "Training: loss 4.531589508056641, covariance difference 0.9995952844619751\n",
      "Validation: loss 5.5842846272352356, covariance difference 32.61960520096131, sinkhorn epsilon 3.762006649556218e-15\n",
      "Iteration  229\n",
      "Training: loss 4.5456953048706055, covariance difference 1.0021129846572876\n",
      "Validation: loss 5.67810599062992, covariance difference 33.1378784862324, sinkhorn epsilon 3.562138614311688e-15\n",
      "Iteration  230\n",
      "Training: loss 4.639527797698975, covariance difference 1.0171895027160645\n",
      "Validation: loss 5.6712144852672886, covariance difference 33.09245518603894, sinkhorn epsilon 2.0508346088583435e-14\n",
      "Iteration  231\n",
      "Training: loss 4.633121013641357, covariance difference 1.0178927183151245\n",
      "Validation: loss 5.69064454222022, covariance difference 33.271948550321525, sinkhorn epsilon 0.0\n",
      "Iteration  232\n",
      "Training: loss 4.651623249053955, covariance difference 1.0215458869934082\n",
      "Validation: loss 5.67475083276199, covariance difference 33.208408821859365, sinkhorn epsilon 6.197404518510298e-14\n",
      "Iteration  233\n",
      "Training: loss 4.636070251464844, covariance difference 1.0173627138137817\n",
      "Validation: loss 5.625223320699749, covariance difference 32.898182937339726, sinkhorn epsilon 1.7918601429961906e-14\n",
      "Iteration  234\n",
      "Training: loss 4.586596488952637, covariance difference 1.0092436075210571\n",
      "Validation: loss 5.654285253912372, covariance difference 33.0184846668515, sinkhorn epsilon 2.4263757879549422e-14\n",
      "Iteration  235\n",
      "Training: loss 4.615236282348633, covariance difference 1.0132118463516235\n",
      "Validation: loss 5.689087392354544, covariance difference 33.203367040255046, sinkhorn epsilon 4.133544395661811e-15\n",
      "Iteration  236\n",
      "Training: loss 4.651290416717529, covariance difference 1.020948052406311\n",
      "Validation: loss 5.657112391850459, covariance difference 32.96268362593474, sinkhorn epsilon 1.6212970326910554e-14\n"
     ]
    }
   ],
   "source": [
    "y_batch = next(data_gen)\n",
    "x_batch = np.random.normal(size=(y_batch.shape[0], LATENT_DIM))\n",
    "res_train = {'loss':{}, 'cov_diff' : {}}\n",
    "res_test = {'loss':{}, 'cov_diff' : {}, 'sample':{}, 'sink_eps':{}}\n",
    "for i in range(ITERS):\n",
    "    out = generator.train_on_batch(x = x_batch,y = y_batch)\n",
    "    res_train['loss'][i] = out[0]\n",
    "    res_train['cov_diff'][i] = out[1]\n",
    "    y_batch = next(data_gen)\n",
    "    x_batch = np.random.normal(size=(y_batch.shape[0], LATENT_DIM))\n",
    "    #validate on the next data\n",
    "    y_sample = generator.predict(x_batch, batch_size=BATCH_SIZE)\n",
    "    res_test['loss'][i], res_test['sink_eps'][i] = sinkhorn_loss_np(y_sample, y_batch, epsilon, CRITIC_ITERS)\n",
    "    res_test['sample'][i] = y_sample\n",
    "    res_test['cov_diff'][i] = np.linalg.norm(np.cov(y_sample) - np.cov(y_batch))\n",
    "    print('Iteration ', i)\n",
    "    print('Training: loss {}, covariance difference {}'.format(res_train['loss'][i], res_train['cov_diff'][i]))\n",
    "    print('Validation: loss {}, covariance difference {}, sinkhorn epsilon {}'.format(\n",
    "        res_test['loss'][i], res_test['cov_diff'][i], res_test['sink_eps'][i]))\n",
    "    if i % 10 == 0:\n",
    "        with open('logs.pkl', 'wb') as f:\n",
    "            pkl.dump([res_test, res_train], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
