Maths and algorithms
=====================

Soon!

If you're interested in computational Optimal Transport, I would strongly recommend you to read
my `tutorial on gradient flows <https://www.math.ens.fr/~feydy/Teaching/DataScience/gradient_flows.html>`_
and the :ref:`notebooks presenting the multiscale Sinkhorn algorithm <sinkhorn-multiscale>`, in the gallery.

Going further
---------------

A comprehensive reference on the algorithms implemented
here will be provided in my **PhD thesis** (~ November 2019) - from CUDA tricks
to theoretical results on Sinkhorn divergences and applications to medical data.
Until then, you may be interested by:

- My `slides <https://www.math.ens.fr/~feydy/Talks/GTTI_2019/GTTI_2019.pdf>`_
  and `poster <https://www.math.ens.fr/~feydy/Talks/AiStats_2019/AiStats_2019_poster.pdf>`_
  on Hausdorff distances, Kernel MMDs and Sinkhorn divergences.
- The `tutorial on gradient flows <https://www.math.ens.fr/~feydy/Teaching/DataScience/gradient_flows.html>`_
  that I wrote for students at the math department of the ENS.
- Our `AiStats2019 paper <https://arxiv.org/abs/1810.08278>`_, which proves
  the **positive-definiteness** and convexity of Sinkhorn divergences.
- Our `ShapeMI2018 paper <https://hal.archives-ouvertes.fr/hal-01827184/>`_, 
  which puts the three geometric loss functions
  in a **common framework**.
- Aude Genevay's `PhD thesis <https://audeg.github.io/publications/these_aude.pdf>`_, 
  which provides an in-depth study of the **statistical properties**
  of Sinkhorn divergences.
- Bernhard Schmitzer's `sparse scaling paper <https://arxiv.org/abs/1610.06519>`_,
  which introduced the first **multiscale Sinkhorn loop**. 




